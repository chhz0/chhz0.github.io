[{"content":" MySQL 事务\n1.事务有哪些特征 原子性，隔离性，一致性，持久性\n原子性：要么全做，要么全不做\n隔离性：保证其它的状态转换不会影响到本次状态的转\n一致性：数据全部符合现实世界的约束\n持久性： 更新后的数据存储到磁盘\nInnoDB引擎通过以下技术来保证事务的四个特性\n持久性是通过 redo log（重做日志）来保证\n原子性是通过 undo log（回滚日志）来保证\n隔离性是通过 mvcc（多版本并发控制）或者锁机制来保证\n一致性是通过持久性+原子性+隔离性来保证\n2.并发事务会引发的问题 MySQL服务端是允许多个客户端连接，这意味着MySQL会出现同时处理多个事务的情况\n在同时处理多个事务的时候，可能会出现脏读、不可重复读、幻读的问题\n脏读：一个事务读到了另一个未提交事务修改过的数据\n不可重复读：在一个事务中多次读取同一个数据，出现前后两次读到的数据不一样的情况\n幻读：在一个事务中多次查询某个符合查询条件的记录数量，如果出现前后两次查询到的记录数据不一样的情况\n以上三个现象，问题的严重性是 脏读 \u0026gt; 不可重复读 \u0026gt; 幻读\n3.事务的隔离级别 四种隔离级别：\n读未提交：指一个事务还没有提交时，它做的变更就能被其他事务看到\n读提交：指一个事务提交之后，它做的变更才能被其他事务看到\n可重复读：指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，MySQL InnoDB引擎的默认隔离级别\n串行化：对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生读写冲突的时候，后访问的事务必须等前一个事务执行完成\n按隔离水平高低排序如下：\n串行化 \u0026gt; 可重复读 \u0026gt; 读已提交 \u0026gt; 读未提交\n针对不同的隔离级别：并发事务时可能发生的现象也不同\n读未提交：脏读、不可重复读、幻读\n读提交：不可重复读、幻读\n可重复读：幻读\n串行化：\n可重复读的隔离级别下，可以很大程度上避免幻读现象的发生，所以MySQL不使用串行化隔离级别来避免幻读现象的发生，因为串行化隔离级别会影响性能\nInnoDB在默认隔离级别：可重复读的情况下很大程度上解决幻读现象的解决方案有两种：\n针对**快照读（普通 select 语句），**是通过MVCC方式解决幻读\n针对**当前读（select \u0026hellip; for update），**通过next-key lock（记录锁+间隙锁）方式解决了幻读\n四种隔离事务是怎么实现的\n对于读未提交：可以读到未提交事务修改的数据，所以直接读取就行\n对于串行化，通过加读写锁的方式来避免并行访问\n对于读提交和可重复读这两种隔离级别的事务，是通过Read View来实现的，它们的区别是在于创建Read View时，读提交隔离级别是在每个语句执行之前都会重新生成一个Read View；而可重复读隔离级别是启动事务时生成一个Read View，然后整个事务都在用这个Read View\n在执行开启事务命令，并不意味着启动了事务：\n在MySQL中，开启事务有两种命令，分别是：\nbegin/start transaction，执行命令后，并不意味着事务启动，只有执行了第一条select语句，才是事务真正启动的时机\nstart transaction with consistent snapshot，马上启动事务\nRead View在MVCC中是如何工作的？ Read View的四个字段\n\u0026lt;creator_trx_id\u0026gt; \u0026lt;m_ids\u0026gt; \u0026lt;min_trx_id\u0026gt; \u0026lt;max_trx_id\u0026gt; creator_trx_id:创建该Read View的事务的事务id\nm_ids:指创建Read View时，当前数据库中活跃事务的事务id列表\nmin_trx_id:生成ReadView时系统中活跃的事务中最小的事务id，即m_ids中的最小的事务id，也是表示活跃的事务最早的那个\nmax_trx_id:表示生成ReadView时系统中应该分配给下一个事务的id值\n这里还需要了解聚簇索引记录中的两个隐藏列，trx_id和roll_pointer\ntrx_id，当一个事务对某条聚簇索引进行改动时，会把该事务的事务id记录在trx_id隐藏列中\nroll_pointer，这个隐藏列是指针，指向每一个旧版本记录\n有了ReadView，以及undo log，在访问某条记录的时，按照以下步骤进行判断：\ntrx_id == creator_trx_id ，意味着当前事务在访问自己修改过的记录，可以访问\ntrx_id \u0026lt; min_trx_id ，表明生成该版本的事务在当前事务生成Read View前已经提交，可以访问\ntrx_id \u0026gt; max_trx_id ，表明生成该版本的事务在当前事务生成Read View后才开启，不可以访问\nmin_trx_id \u0026lt; trx_id \u0026lt; max_trx_ix ，还需要再进一步判断\ntrx_id 存在 m_ids 中，说明创建Read View时生成该版本的事务还是活跃的，不可以访问\ntrx_id 不在 m_ids 中，说明创建Read View时生成该版本的事务已经提交了，可以访问\nRead COMMITTD、REPETABLE READ这两种隔离级别的一个很大不同：生成ReadView的时机不同，REAED COMMITTD在每一次进行普通select操作前都会生成一个ReadView，而REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，之后的查询操作都重复使用这个ReadView，不会再生成一个新的ReadView\n这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。\n可重复读是如何工作的？ 可重复读隔离级别是启动事务时生成一个Read View，然后整个事务期间都在用这个Read View\n如果某个事物开启后，去读取记录，发现记录的trx_id 比自己事物id小且在活跃的事物id列表里面有该事务id，那么该事务不会读取该版本的记录，而是沿着undo log链条往下找旧版本的记录，直到找到trx_id比事务b小的min_trx_id值的第一条记录\n读提交是如何工作的？ 读提交隔离事件在每次读取数据的时候，都会生成一个新的Read View\n","date":"2025-07-03","id":0,"permalink":"/notes/database/mysql/mysql-transaction/","summary":"MySQL 事务","tags":"MySQL","title":"Mysql Transaction(bate)"},{"content":" MySQL 日志 - undo log | redo log | bin log\n先理解执行一条sql语句，在mysql内部会发生什么？\n以执行一条update 语句为例：\n客户端会先通过连接器建立连接，连接器会判断用户身份\n这里是一条update语句，所以不需要经过查询缓存（注意，当表上有更新语句，会把整个查询缓存清空，所以在Mysql8.0这个功能就被移除了）\n解析器会通过词法分析识别出关键字，构建出语法树，接着做语法分析，判断输入的语句是否符合MySQL语法\n预处理器会判断表和字段是否存在\n优化器确定执行计划（使用索引或者全表查询）\n执行器负责具体执行，找到这一行然后更新\n不过，更新语句的流程会涉及到undo log**，redo log，binlog**三种日志：\nundo log（回滚日志）：是InnoDB存储引擎生成的日志，实现了事务中的原子性，主要用于事务回滚和MVCC\nredo log（重做日志）：是InnoDB存储引擎生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复\nbing log（归档日志）：是Server层生成的日志，主要用于数据备份和主从复制\n1.为什么需要undo log？ 在执行一条“增删改”语句的时候，MySQL会隐式开启事务，执行完后自动提交事务\nMySQL中执行一条语句后是否自动提交事务，是由autocommit 参数来决定的，默认是开启的\n当事务执行过程中，都记录下回滚时需要的信息到一个日志中，那么在事务执行过程中发生MySQL崩溃后，可以通过这个日志回滚到事务之前的数据\n实现这一机制就是 undo log**（回滚日志），它保证了事务的ACID特性中的原子性**\n每当InnoDB引擎对每种操作进行回滚时，进行相反操作就行：\n插入 - 删除\n删除 - 插入\n更新 - 更新为旧值\n一条记录每次进行操作产生的undo log格式都有一个roll_pointer和一个trx_id事务id：\ntrx_id：记录该记录是被哪些事务修改的\nroll_pointer：指针可以将这些undo log串成一个链表，这个链表被称为版本链\n另外，undo log可以跟Read View一起实现MVCC（多版本并发控制）：\n对于 读提交 和 可重复读 隔离级别的事务来说，它们的快照读（普通select语句）是通过Read View + undo log来实现的，区别在于创建Read View的时机不同\n读提交：是在每一个select都会生成一个新的Read View，也意味着事务期间的多次读取同一数据，前后两次读的数据可能会出现不一致（不可重复读）\n可重复读：是在启动事务时生成一个Read View，然后整个事务期间都在用这个Read View，这样保证了事务期间读到的数据都是事务启动时的记录\n这两个隔离级别实现是通过事务的Read View里的字段和记录两个隐藏列trx_id和roll_pointer的对比\n事务隔离级别是怎么实现的？\n因此，undo log两大作用：\n实现事务回滚，保障事务的原子性\n实现MVCC（多版本并发控制）关键因素之一\nUndo log是如何刷盘？\nUndo log和数据页的刷盘策略是一样的，都需要通过redo log保证持久化\nBuffer pool中有undo 页，对undo页的修改都会被记录到redo log。redo log每秒刷盘，提交事务时也会刷盘，数据页和undo 页都是靠这个机制保证持久化\n2.为什么需要Buffer Pool？ MySQL的数据都是存储在磁盘中的，那么我们更新一条记录，得先从磁盘读取该记录，然后在内存中修改记录，修改完之后并不会直接写回磁盘，而是缓存起来，这样下次查询语句命中这条记录，就不需要从磁盘读取数据\n为此，InnoDB存储引擎设计了一个**缓冲池****Buffer Pool，**来提高数据库的读写性能\n暂时无法在飞书文档外展示此内容\n有了Buffer Pool后：\n当读取数据时，如果数据存在于Buffer Pool中，客户端会直接读取Buffer Pool中的数据，否则再去磁盘中读取\n当修改数据时，如果数据存在Buffer Pool中，那么直接修改Buffer Pool中数据所在的页，然后将页设置为脏页****（该页上的数据和磁盘上的不一样），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择以一个合适的时机将脏页写入到磁盘\nBuffer Pool缓冲了什么？ InnoDB会把存储的数据划分为若干个页，以页为磁盘和内存交互的基本单位，一个页的默认大小为16K\nBuffer Pool同样按 页 来划分\n在MySQL启动的时候，InnoDB会为Buffer Pool申请一片连续的内存空间，然后按照默认的**16k** 的大小划分一个个的页，Buffer Pool中的页叫做缓冲页。\nBuffer Pool除了缓存索引页和数据页，还包括Undo页，插入缓存，自适应哈希索引，锁信息等\nUndo 页是记录什么？\n开启事务后，InnoDB层更新记录前，首先要记录相应的undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是生成一个undo log，undo log会写入到undo页面\n查询一条记录，就只需要缓存一条记录吗？\n不是，当查询一条记录的时候，InnoDB会将整个页的数据加载到Buffer Pool中，将页加载到Buffer Pool后，再通过页里的页目录去定位到某条具体的记录 换一个角度看B+树\n3.为什么需要 redo log？ Buffer Pool是提高了读写效率，但是Buffer Pool是基于内存的，而内存总是不可靠，出现断电重启时内存里没来得及落盘的脏页数据就会丢失\n为了防止断电导致数据丢失，当一条记录需要更新，InnoDB就会先更新内存（同时标记为脏读页），然后将本次对这个页的修改以redo log的形式记录下来，这时候才算更新完成\n后续，InnoDB引擎会在适合的适合，由后台线程将Buffer Pool的脏页刷新到磁盘里，这个就是WAL****（Write-Ahead-Logging）技术\nWAL技术指的是，MySQL的写操作并不是立刻写到磁盘上面，而是先写日志，然后在合适的时间再写到磁盘上。\n什么是redo log？ redo log是物理日志，记录了某个数据页做了什么修改，每当执行一个事务就会产生这样的一条或者多条物理日志\n在事务提交的时候，只要先将redo log持久化到磁盘即可，可以不需要等待到将缓存在Buffer Pool里的脏页数据持久化到磁盘\n当系统崩溃时，虽然脏页数据没有持久化，但是redo log语句持久化了，接着Mysql重启后，可以根据redo log的内容，将所有数据恢复到最新的状态\n被修改undo页面，需要记录对应redo log吗？ 需要的。\n开启事务后，InnoDB层要更新记录前，首先要记录相应的undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是生成一条undo log，undo log会写入Buffer Pool中的Undo页面\n不过，在内存修改该undo页面后，需要记录对应的redo log\nredo log和undo log区别在哪里？ 这两种日志都是InnoDB引擎下的日志，它们的区别在于：\nredo log记录了事务完成后的数据状态，记录的是更新之后的值；\nundo log记录了事务开始前的数据状态，记录的是更新之前的值；\n当事务提交之前发生崩溃，重启后会通过undo log回滚事务，事务提交之后发生崩溃，重启后会通过redo log恢复事务\n暂时无法在飞书文档外展示此内容\n有了redo log，再通过WAL技术，InnoDB可以保证即使数据库发生异常重启后，之前已提交的记录都不会丢失，这个能力就是 crash-safe（崩溃恢复）\nredo log****保证了事务四大特性中的持久性\nredo log要写磁盘，数据也要写磁盘，为什么要多次一举？ 写入redo log的方式是由了追加操作，所以磁盘操作是顺序写，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是随机写\n磁盘的顺序写比随机写要高效得多，因此redo log写入磁盘的开销更小\n可以说，这是WAL技术的另外一个优点：MySQL****的写操作从磁盘的随机写变成了顺序写\n至此，针对为什么需要redo log这个问题我们有两个答案：\n实现事务的持久性，让MySQL有crash-safe能力\n将写操作从随机写到顺序写，提高MySQL写入磁盘的性能\n产生的redo log是直接写入磁盘的吗 不是，redo log也有自己的缓存——redo log buffer，每当产生redo log时，会先写入到redo log buffer，后续再持久化到磁盘：\nRedo log buffer默认的大小是16MB，可以通过innodb_log_Buffer_size 参数动态的调整大小，增大它的大小可以让MySQL处理大事务时不必写入磁盘，进而提高IO性能\nredo log什么时候刷盘？ 缓存在redo log buffer里的redo log还是在内存中，它会在以下的时机刷新到磁盘\nMySQL正常关闭时\n当redo log buffer中记录的写入量大于redo log buffer内存空间一半时，就会触发落盘\nInnoDB的后台线程每隔1s，将redo log buffer持久到磁盘\n每次事务提交时都将缓存在redo log buffer里的redo log直接持久化到磁盘（这个策略可以通过innodb_flush_log_at_trx_commit 参数控制）\ninnodb_flush_log_at_trx_commit 参数控制的时候什么？ 设置参数为0时，表示每次事务提交时，还是将redo log留在redo log buffer，该模式下事务提交时不主动触发写入到磁盘的操作\n设置参数为1时，表示每次事务提交时，都将缓存在redo log buffer里的redo log直接持久化到磁盘，这样可以保证MySQL异常重启之后的数据不会丢失\n设置参数为2时，表示每次事务提交时，都只是缓存在redo log buffer里的redo log写到redo log文件，注意写入到 redo log文件并不意味着写入到了磁盘，而是写入到了Page Cache，就行写入到了操作系统的文件缓存\ninnodb_flush_log_at_trx_commit 为0和2时，什么时候才将redo log写入磁盘？ InnoDB的后台线程每隔1s：\n针对参数0：会把缓存在redo log buffer中的redo log，通过write()写到操作系统的Page cache，然后调用fsync() 持久化到磁盘。所以参数为0的策略，MySQL进程崩溃会导致上一秒所有事务数据的丢失\n正对参数2：调用fsync() ，将缓存在操作系统中Page Cache里的redo log持久化到磁盘，所以参数为2的策略，较取值为0情况下更安全，因为MySQL进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒所有的事务数据才可能丢失\ninnodb_flush_log_at_trx_commit 三个参数的应用场景是什么？ redo log 文件写满了怎么办？ 默认情况下，innoDB存储引擎有一个重做日志文件组（redo log Group），重做日志文件组由2个redo log文件组成，这两个redo 日志的文件名叫：ib_logfile0 和ib_logfile1\n在重做日志组中，每个redo log file的大小都是固定且一致的\n重做日志组是以循环写的方式工作，从头开始写，写到末尾就回到开头，相当于一个环形\nInnoDB存储引擎会先写ib_logfile0文件，当ib_logfile0文件被写满的时候，会切换至ib_logfile1文件，1写满时会被切换到0文件\nredo log是为了防止Buffer Pool中的脏页丢失而设计的，那么随着系统运行，Buffer Pool的脏页刷新到了磁盘，那么redo log对应的记录也就没有了\nredo log是循环写的方式，相当于一个环形，InnoDB用Write pos表示redo log当前记录写到的位置，用checkpoint表示当前要擦除的位置\nwrite pos和check point的移动都是顺时针方向\nwrite pos ~ check point之间的部分（红色），用来记录新的更新记录\ncheck point ~ write pos 之间的部分（蓝色），待落盘的脏数据页记录\n如果write pos追上checkpoint，就意味着redo log文件满了，这时MySQL不能再执行新的更新操作，也就是MySQL会发生阻塞（因此针对并发量大的系统，适当增大redo log文件的大小非常重要），此时会停下来将Buffer Pool中的脏页刷新到磁盘中，然后标记redo log哪些记录可以被擦除，接着对旧的redo log记录进行擦除，等擦除完旧记录腾出空间，checkpoint就会往后移动，MySQL恢复正常运行，继续执行新的更新操作\n4.为什么需要binlog？ 前面的undo log和redo log都是InnoDB存储引擎生成的日志\nMySQL在完成一条更新操作后，Server层还会生成一条binlog，等之后事务提交的时候，会将事务执行过程中产生的所有binlog统一写入binlog文件\nbinlog文件记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作\n为什么有了binlog，还要redo log？\n因为最开始MySQL只有MyISAM引擎，没有InnoDB引擎，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档\n而InnoDB是另外一个公司以插件的形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用redo log来实现crash-safe能力\nredolog 和 binlog有什么区别？ Redolog 和 binlog有四个区别：\n适用对象不同：\nbinlog 是MySQL的Server层实现的日志，所有的引擎都可以使用\nredolog 是InnoDB存储引擎实现的日志\n文件格式不同：\nbinlog有3种格式类型，分别是STATEMENT、ROW、MIXED区别如下：\nSTATEMENT：每一条修改数据的SQL都会记录到binlog中（相当于记录了逻辑操作，所以针对这种格式，binlog可以称为逻辑日志）\nROW：\nMIXED：\nredolog是物理日志，记录的是在某个数据页做了什么修改。\n写入方式不同：\nbinlog是追加写，写满一个文件，就创建一个新文件继续写，不会覆盖以前的日志，保存的是全量的日志\nredolog是循环写，日志空间大小是固定的，全部写满从头开始，保存未被刷入磁盘的脏页日志\n用途不同：\nbinlog用于备份恢复，主从复制\nredolog用于掉电等故障恢复\n不小心整个数据库的数据删除了，能用redo log文件恢复数据吗？\n不可以使用redo log文件恢复，只能使用binlog文件恢复\n因为redo log文件是循环写，是会边写边擦日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会被从redolog文件里擦除\nbinlog文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在binlog上的数据，都可以恢复\n主从复制是怎么实现的？ MySQL的主从复制依赖于binlog，记录MySQL上的所有变化以二进制形式保存在磁盘上，复制的过程就是将binlog中的数据从主库传输到从库上\n这个过程一般是异步的\nMySQL集群的主从复制过程大致三阶段：\n写入binlog：主库写binlog日志，提交事务，并更新本地存储数据\n同步binlog：把binlog复制到所有从库上，每个从库把binlog写到暂存日志中\n回放binlog：回放binlog，并更新存储引擎中的数据\n具体详细过程如下：\nMySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。\n从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。\n从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。\n从库是不是越多越好\n不是，从库的数据增加，从库连接上来的I/O线程就越多，主库建立同样多的log dump线程来处理复制的请求，对主库资源消耗比较高，同时还限制于主库的网络带宽\n在实际使用中，一般一主跟2~3从库\nMySQL主从复制还有哪些模型\n同步复制\n异步复制\n半同步复制\nbinlog是什么时候刷盘？ 5.为什么需要两阶段提交？ 两阶段提交的过程是怎样的？ 重启异常会出现什么现象？ ","date":"2025-07-03","id":1,"permalink":"/notes/database/mysql/mysql-log/","summary":"MySQL 日志 - undo log | redo log | bin log","tags":"MySQL","title":"Mysql Log(bate)"},{"content":" MySQL 锁 - 全局锁|表级锁|行级锁\n锁的类型 Mysql的锁，根据加锁的范围可以分为全局锁、表级锁和行锁三类\n全局锁 要使用全局锁，执行下面这条命令：\nflush tables with read lock 执行之后，整个数据库就处于只读状态，这时其他线程执行以下操作，就会被阻塞\n对数据的增删改，比如insert、delete、update等\n对表结构的更改操作，比如alter table、drop table等\n要释放全局锁，执行下面的命令：\nunlock tables 全局锁的应用场景：\n全局锁主要用于做全库逻辑备份，这样在备份数据库期间，不会因为数据或者结构的更新，而出现备份文件的数据与预期的不一样\n加全局锁带来的缺点：会导致业务停滞，因为加全局锁之后，整个数据库都只是只读状态，不能更新数据\n可以通过开启事务，在可重复读的隔离级别下，即使其他事务更新了表的数据，也不会影响备份数据库时的Read View，\n备份数据库的工具是mysqldump ，在使用mysqldump时加上-single-transaction 参数的时候，就会在备份数据库之前开启事务\n表级锁 MySQL里面表级锁有以下几种：\n表锁\n元数据锁（MDL）\n意向锁\nAUTO-INC锁\n表锁 使用下面的命令对表加锁和释放锁\n// 加读锁 lock tables \u0026lt;table_name\u0026gt; read; // 写锁 lock tables \u0026lt;table_name\u0026gt; write; // 释放锁 unlock tables; 表锁会影响别的线程和本线程的读写操作\n元数据锁（MDL） 对于MDL，我们不需要显示使用，因为当我们在对数据库进行操作时，会自动给这个表上加MDL：\n对一张表进行CURD操作时，加的是MDL读锁\n对一张表做结构变更操作的时候，加的是MDL写锁\nMDL是为了保证当前用户对表执行CRUD操作时，防止其他线程对这个表结构做了变更\nMDL是在事务提交之后才会释放，这意味着事务执行期间，MDL是一直持有\n需要注意的是，在事务启用之后，如果事务A没有提交，此时如果有表结构的修改请求发起，就会发生阻塞，这个阻塞也会导致其他CURD的请求被阻塞住\n这是因为申请MDL锁的操作会形成一个队列，队列中写锁获取优先级大于读锁，一旦出现MDL写锁等待，会阻塞该表后续的CRUD操作\n意向锁 在使用InnoDB引擎的表里对某些记录加上共享锁之前，需要先在表级别加上一个意向共享锁\n在使用InnoDB引擎的表里对某些记录加上独占锁之前，需要先在表级别加上一个意向独占锁\n在执行insert、update、delete操作时，需要先对表上加 意向独占锁，然后对该记录加独占锁\n而普通的select是不会加行级锁，普通的select语句是利用MVCC实现一致性读，是无锁的\n// select也是可以对记录加共享锁和独占锁， // 先在表上加上意向共享锁，然后对读取的记录加共享锁 select ... lock in share mode; // 先表上加上意向锁，然后再读取记录加独占锁 select ... for update 意向锁的目的是为了快速判断表里是否有记录被加锁\nAUTO-INC锁 表里面的主键通常设置成自增的，在插入数据时，可以不指定主键的值，数据库会自动给主键赋值递增的值，这主要是通过AUTO-INC锁实现的\nAuto-Inc锁是特殊的表锁机制，不是在一个事务提交后才释放，而是再执行完插入语句后就会立即释放\n行级锁 InnoDB引擎是支持行级锁的，而MyISAM引擎并不支持行级锁\n行级锁的类型主要有三类：\nRecord Lock，记录锁，也就是仅仅一条记录锁上\nGap Lock，间隙锁，锁定一个范围，但不包含记录本身\nNext-key Lock，Rocord Lock + Gap Lock的组合，锁定一个范围，并且锁定记录本身\nRecord Lock 记录锁 Record Lock称为记录锁，锁住的是一条记录。而且记录锁也有s锁和x锁之分\nGap Lock 间隙锁 Gap Lock称为i而间隙锁，只存在于可重复隔离级别，目的是为了解决可重复读级别下幻读的现象\n间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不会存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出\nNext-key Next-key Lock成为临键锁，是Record Lock + Gap Lock的组合，锁定一个范围，并且锁定记录本身\nNext-key Lock是包含间隙锁+记录锁，如果一个事务获取了X型的next-key lock，那么另外一个事务在获取相同范围的X型的next-key lock时，是会被阻塞的\n插入意向锁 在一个事务插入一条记录的时候，需要判断插入的位置释放已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。\n如果有，插入操作就会阻塞，直到间隙锁被释放，在此期间会生成一个插入意向锁，表明有事务想在某区域插入新记录，但是处于等待状态\nMySQL加锁 InnoDB引擎是支持行级锁，而MyISAM是不支持行级锁，了解Mysql是怎么加行级锁，其实也是说InnoDB引擎是怎么加锁的。\n普通select语句是不会对记录加锁（除了串行化隔离级别），因为它属于快照读，是通过MVCC（多版本并发控制）实现的\n对查询时对记录加行级锁，可以使用下面两种方式，这两种查询会加锁的语句叫做锁定读\n// s lock select ... lock in share mode; // x lock select ... for update; 上面两个语句必须在事务中，因为当事务提交了，锁就释放。\n除了上面两条锁定读语句会加行级锁之外，update和delete操作都会加行级锁，且锁定类型都是独占锁(X)\nupdate table ... delete from table ... x型锁和s型锁之间的兼容型未读读共享，读写互斥\nX S X 不兼容 不兼容 S 不兼容 兼容 行级锁\n读已提交隔离级别下，行级锁的种类只有记录锁，也就是仅仅一条记录锁上\n可重复读隔离级别下，行级锁的种类除了记录锁，还有间隙锁（目前是为了避免幻读\n在执行commit后，事务过程中生成的锁都会被释放\nMySQL是怎么加行级锁的？ 行级锁加锁规则复杂，目前仅保留了解程度\nMySQL 是怎么加锁的？\n总结1：在能够使用记录锁或者间隙锁就能避免幻读的现象的场景下，next-key lock就会退化成为记录锁或间隙锁\nMysql死锁 使用引擎为InnoDB，隔离级别为可重复读（RR）\n死锁的发生 有表如下：\ncreate table `t_order` ( `id` int not null auto_increment, `order_no` int default null, `create_date` datetime default null, primary key(`id`), key `index_order` (`order_no`) using btree ) engine = InnoDB; 有两个事务，一个事务要插入订单1007，另外一个事务也要插入订单1008，因为需要对订单做幂等性校验，所以两个事务先要查询订单是否存在，不存在才插入记录\n这里两个事务都陷入阻塞（前提是没有打开死锁检测），也就是发生了死锁，都在互相等待对方释放锁\n死锁的产生 可重复隔离级别下，是存在幻读的问题\nInnoDB引擎为了解决可重复读隔离级别下的幻读问题，就引出了next-key锁，它是记录锁和间隙锁的组合\nRecord Lock，记录锁，锁的是记录本身\nGap Lock，间隙锁，锁的是两个值之间的空隙，以防止其他事务在这个空隙间插入新的数据，从而避免幻读\n普通的select是通过mvcc实现的快照读，不会对记录进行加锁，如果要在查询的时候加行锁，可以使用下面的两种方式\nbegin; // 对读取的记录加共享锁 select ... lock in share mode; commit; begin; // 对读取的记录加排他锁 select ... for update; commit; 行锁的释放时机是在事务提交（commit）后，锁才会释放，并不是在一条语句执行完毕之后释放锁\nselect * from performance_schema.data_lock\\G; 执行以上的语句，可以查看事务执行SQL过程中加了什么锁\nLOCK_TYPE 中的RECORD表示行级锁，通过LOCK_MODE 可以确认是next-key锁，间隙锁还是记录锁\nLOCK_MODE: X ，说明是X型的next-key锁；\nLOCK_MODE: X, REC_NOT_GAP ，说明是X型的记录锁；\nLOCK_MODE: X, GAP ，说明是X型的间隙锁；\n当事务B往事务A next-key锁的范围插入记录时，就会被锁住\n执行插入语句时，会在插入间隙上获取插入意向锁，而插入意向锁与间隙锁是冲突的，所以当其它事务持有该间隙的间隙锁时，需要等待其它事务释放间隙锁之后，才能获取到插入意向锁。而间隙锁与间隙锁之间是兼容的，所以两个事务中**select ... for update** 语句并不会相互影响。\n这样，事务A和事务B在执行完select ... for update 语句之后都持有了间隙锁，而接下来的insert 操作为了获取到插入意向锁，都在等待对方事务的间隙锁释放，于是就造成了循环等待，导致死锁。\n为什么间隙锁和间隙锁之间是兼容的？\nMySQL官网上有描述：\nGap locks in InnoDB are “purely inhibitive”, which means that their only purpose is to prevent other transactions from Inserting to the gap. Gap locks can co-exist. A gap lock taken by one transaction does not prevent another transaction from taking a gap lock on the same gap. There is no difference between shared and exclusive gap locks. They do not conflict with each other, and they perform the same function.\n间隙锁的意义只在于阻止区间被插入，因此可以共存。**一个事务获取的间隙锁不会阻止另外一个事务获取同一个间隙范围的间隙锁，**共享和排他的间隙锁是没有区别的，它们相互不冲突，且功能相同，即两个事务可以共同持有包含共同间隙的间隙锁\n共同间隙包括两种场景：\n两个间隙锁的间隙区间完全一样\n一个间隙包含的间隙区间是另外一个间隙锁区间的子集\n注意：next-key lock是包含间隙锁+记录锁的，如果一个事务获取了X型的next-key lock，那么另外一个事务在获取相同范围的X型的next-key lock时，是会被阻塞的\n再注意：对于右区间为+∞的next-key lock，因为+∞并不是一个真实的记录，所以我不需要考虑X型和S型\n插入意向锁是什么？\nMySQL的描述：\nAn Insert intention lock is a type of gap lock set by Insert operations prior to row Insertion. This lock signals the intent to Insert in such a way that multiple transactions Inserting into the same index gap need not wait for each other if they are not Inserting at the same position within the gap. Suppose that there are index records with values of 4 and 7. Separate transactions that attempt to Insert values of 5 and 6, respectively, each lock the gap between 4 and 7 with Insert intention locks prior to obtaining the exclusive lock on the Inserted row, but do not block each other because the rows are nonconflicting.\n这段话表明尽管插入意向锁是一种特殊的间隙锁，但不同于间隙锁的是，该锁只用于并发插入操作。\n如果说间隙锁锁住的是一个区间，那么插入意向锁锁住的是一个点，因而从这个角度来说，插入意向锁确实是一种特殊的间隙锁\n插入意向锁的生成时机：\n每插入一条新纪录，都需要看一下待插入记录的下一条记录上是否已经被加了间隙锁，如果已加间隙锁，此时会生成一个插入意向锁，如何锁的状态设置为等待状态，现象就是Insert语句会被阻塞（_PS：_MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁） Insert语句是怎么加行级锁的？ Insert语句在正常执行时是不会生成锁结构的，它是靠聚簇索引记录自待的trx_id 隐藏列来作为隐式锁来保护记录的\n什么是隐式锁？\n当事务需要加锁时，如果这个锁不可能发生冲突，InnoDB会跳过加锁环节，这种机制称为隐式锁\n隐式锁是InnoDB实现的一种延迟加锁机制\n如何避免死锁？ 死锁的四个必要条件：\n互斥\n占有且等待\n不可强占用\n循环等待\n只要发生死锁，这些条件必然成立，但是只要破环其中一个条件死锁就不会成立\n在数据库层面，有两种策略通过打破循环等待条件来解除死锁状态：\n设置事务等待锁的超时时间：当一个事务的等待时间超过该值后，就对这个事务进行混滚，于是锁就释放了，另外一个事务就可以继续执行了。在InnoDB中，参数innodb_lock_wait_timeout 是用来设置超时时间的，默认时间为50s\n开启主动死锁检测：主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将innodb_deadlock_detect 设置为on，表示开启这个逻辑，默认就开启。\n","date":"2025-07-03","id":2,"permalink":"/notes/database/mysql/mysql-lock/","summary":"MySQL 锁 - 全局锁|表级锁|行级锁","tags":"MySQL","title":"Mysql Lock(bate)"},{"content":" MySQL 索引\n1. B+树（索引数据结构） 什么是索引？ 为什么索引能加快查询？ 索引的数据结构是什么？ B+ 树 和（B 树 和 红黑树）有什么区别？ 为什么选择 B+树 作为索引数据结构？\n为什么Mysql InnoDB选择B+ Tree作为索引？ B+ 树 vs B 树 B+ 树只在叶子节点存储数据，B树的非叶子节点也要存储数据，所以B+ 树的单个节点的数据量更小 B+ 树 vs 二叉树 对于有N个叶子节点的B+ 树，搜索复制度为O（logdn） B+ 树 vs Hash 08 索引:排序的艺术\n为什么 MySQL 采用 B+ 树作为索引？\n2. 索引组织表（索引存储） 堆表和索引组织表有什么区别？\n分别应用场景是什么？\nMysql InnoDB存储引擎中数据存储方式：索引组织表\n数据存储有堆表和索引组织表两种。\n堆表中的数据是无序存放的，数据的排序完全依赖索引\n索引组织表，数据根据主键进行排序存放在索引中，主键索引也叫聚集索引（Clustered Index）\n在索引组织表中，数据即索引，索引即数据\n二级索引 InnoDB存储引擎的数据是根据主键索引排序存储的，除了主键索引外，其它的索引都称为二级索引（Secondeary Index），或者非聚集索引\n二级索引也是一颗B+树索引，但是它和主键索引不同的是叶子节点存放的是索引键值、主键值\n通过二级索引idx_name 只能定位主键值，需要额外再通过主键索引进行查询，才能得到最终结果。\n这种二级索引通过主键索引进行再一次查询的操作叫做“回表”\n这样的二级索引设计的好处：若记录发生了修改，则其它索引无须进行维护，除非记录的主键发生了修改\n在索引组织表中，万物皆索引，索引就是数据，数据就是索引。\n二级索引的性能评估 要比较顺序，对聚集索引性能友好\n尽可能紧凑，对二级索引的性能和存储友好\n函数索引（先了解） \u0026hellip;\n09 索引组织表:万物皆索引\n3.组合索引（联合索引） 联合索引的结构是什么？\n如果利用联合索引提升查询性能\n组合索引（Compound Index）是指由多个列所组合而成的B+树索引\n组合索引既可以是主键索引，也可以是二级索引，只是排序的键值从1个变成了多个，本质还是一棵B+树索引\n索引覆盖 目的是为了避免回表，由于二级组合索引的叶子节点，包含索引键值和主键值，若查询的字段在二级索引的叶子节点中，则可以直接返回结果，无需回表。\n这种组合索引避免回表的优化手段称为索引覆盖（Covering Index）\n10 组合索引:用好，性能提升 10 倍!\nMySQL夜市8月25日（联合索引）\n4.索引失效 有哪些索引失效的场景？\n为什么会失效？\n前提：索引可以提高语句查询速度，但是索引并不是万能的，建立了索引，并不意味着任何查询语句都能走索引扫描\n索引存储结构长什么样？ MySQL默认的存储引擎是InnoDB，采用的是B+树作为索引的数据结构。在建表的时候，InnoDB存储引擎默认会创建一个主键索引，也就是聚簇索引，其它索引都属于二级索引\n失效情况 A. 对索引使用左或者左右模糊匹配 索引B+树是按照索引值有序存储的，只能根据前缀进行比较\nB. 对索引使用函数 索引保存的是索引字段的原始值，而不是经过函数计算后的值\nC. 对索引进行表达式计算 原因与索引使用函数差不多，进行了表达式计算后得到的值不是原本的值，无法走索引\nD. 对索引隐式类型转换 Mysql的类型转换规则：\n字符串 \u0026ndash;\u0026gt; 数字，就相当于 数字比较\n数字 \u0026ndash;\u0026gt; 字符串，就是字符串比较\n小总结：在Mysql中，遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较\nE. 联合索引非最左匹配 F. Where 子句中的OR 在WHERE子句中，如果在OR前的条件列是索引列，而OR后面不是索引列，那么索引会失效\n索引失效有哪些？\nB+树里面的节点存放的是什么？查询数据的结果又是怎样的？\n5.索引选择 Mysql数据库中的优化器是怎么执行的？\n根据什么标准选择索引？\nMySQL是如何选择所索引的？ 在关系型数据库中，B+树索引只是存储的一种数据结构，具体使用还需要依赖数据库的优化器，优化器决定了具体某一索引的选择\n而优化器的选择是基于成本（cost），哪个索引的成本越低，优先选择哪个索引\nCost = Server Cost + Engine Cost = CPU Cost + IO Cost 先看MySQL数据库的结构，MySQL由Server层和Engine层组成：\nServer层有SQL分析器、SQL优化器、SQL执行器，用于负责SQL语句的具体执行过程\nEngine层负责存储具体的数据，常使用InnoDB存储引擎，还有用于内存中存储临时结果集的TempTable引擎\nMySQL索引出错：\n未使用创建的索引\n索引创建在有限状态上\n11 索引出错:请理解 CBO 的工作原理\n6.索引应用 建立索引有什么优点和缺点？\n如何正确使用索引？\n哪些场景下适合建立索引？\n哪些场景下不适合建立索引？\n总结 B+树索引\n索引的加快查询的一种数据结构，其原理是插入时对数据排序，缺点是会影响插入的性能\nMysql当前支持B+树索引、全文索引、R树索引\nB+树索引的高度通常为3~4层，高度为4的B+树可以存放50亿左右的数据\n由于B+树的高度不高，查询效率高，50亿的数据也只需插叙4次I/O\nMysql单表的索引没有个数限制，业务查询需要，创建即可\n可以通过表sys.schema_unused_indexes和索引不可见特性，删除无用的索引\nMysql采用B+树索引？从数据结构、磁盘I/O操作次数出发\n索引组织表\nMysql InnoDB存储引擎是索引组织表，以及索引组织表和堆表之间的区别：\n索引组织表主键是聚集索引，索引的叶子节点存放表中一整行完整记录\n除主键索引外的索引都是二级索引，索引的叶子节点存放的是（键值，主键值）\n由于二级索引不存放完整记录，因此需要通过主键值再进行一次回表才能定位到完整数据\n索引组织表对比堆表，在海量并发的OLTP业务中能有更好的性能表现\n每种不同数据，对二级索引的性能开销影响不一样\n有时通过函数索引可以更快解决线上SQL的性能问题\n虚拟列不占用实际存储空间，在虚拟类上创建索引本质就是函数索引\n组合索引\n组合索引也是一颗B+树，只是索引的列由多个组成，组合索引既可以是主键索引，也可以是二级索引\n组合索引的三大优势\n覆盖多个查询条件，如（a，b）索引可以覆盖查询 a = ? 或者 a = ? and b = ?\n避免SQL的额外排序，提高SQL性能，如WHERE a = ? OR ORDER BY b 这样的查询条件\n利用组合索引包含多个列的特性，可以利用索引覆盖技术，提高SQL的查询性能，用好索引覆盖技术，性能提升10倍不是难事\n索引失效\n6种会发生索引失效的情况：\n使用左或者左右模糊匹配的时候，也就是 like %xx 或者like %xx% 这两种方式，都会造成索引失效\n当我们查询条件中对所有列使用函数，会导致索引失效\n在查询条件中对所有列进行表达式运算，会导致索引失效\nMySQL遇到字符串和数字比较的时候，会自动把字符串转为数字，再进行比较。如果字符串是索引列，而条件语句中的输入参数是数字的话，那么所有列会发生隐式类型转换，由于隐式类型转换是通过CAST函数实现的，等于对索引列使用了函数，所以导致索引失效\n联合索引要能正确使用遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则索引会失效\n在WHERE子句中，如果在OR前的条件列是索引列，而在OR后的条件列不是索引列，那么所有会失效\n索引选择\nMySQL优化器是CBO，是一种基于成本的优化器。会判断每个索引的执行成本，从中选择出最优的执行计划\nMySQL优化器是CBO（Cost-based Optimizer）的\nMySQL会选择成本最低的执行计划，可以通过explain命令查看每个SQL的成本\n一般只对高选择度的字段和字段组合起来建立索引，选择度低的字段如性别，不建议建立索引\n若数据存在倾斜，可以创建直方图，让优化器知道索引中的数据的分布，进一步校准执行计划\n面试题： 1.为什么InnoDB选择B+Tree作为索引的数据结构？ B+树 vs B树\n存储相同数据量级下，B+树高比B树低，磁盘I/O次数更少\nB+树叶子节点用双向链表串起来，适合范围查询，B树无法做到这点\nB+树 vs 二叉树\n随着数据量的增加，二叉树的树高会越来越高，磁盘I/O次数也会更多，B+树在千万级别的数据量下，高度依然维持在3~4层左右 B+树 vs Hash\n虽然Hash的等值查询效率高，但是无法做到范围查询 2.什么时候适用索引？ 字段有唯一限制性\n经常用于WHERE查询条件\n经常用于GROUP BY 和 ORDER BY的字段\n3.什么时候不需要创建索引？ WHERE条件，GROUP BY，ORDER BY里用不到的字段\n字段中存在大量重复数据\n表数据太少\n经常需要更新的字段\n4.什么时候索引会失效？ 左或左右模糊匹配\n在查询条件中对索引列做了计算、函数、类型转换等操作\n联合索引要正确遵循最左匹配原则\n在WHRER子句中，如果在OR前的条件是索引列而OR后的条件列不是索引列\n为了更好使用索引，索引列要设置为NOT NULL\n5.有什么优化索引的方法？ 回答：\n前缀索引优化\n覆盖索引优化\n主键索引最好是自增的\n防止索引失效\n#MySQL\n","date":"2025-07-03","id":3,"permalink":"/notes/database/mysql/mysql-index/","summary":"MySQL 索引","tags":"MySQL","title":"Mysql Index(bate)"},{"content":" MySQL 缓存池\n为什么要有Buffer Pool MySQL的数据存储在磁盘的，如果每次都从磁盘里面读取数据，这样性能是很差的\n提高性能，就需要加入缓存。当数据从磁盘中取出来之后，缓存内存中，下次查询同样的数据，直接从内存中读取\n为此InnoDB存储引擎设计了一个缓存池（Buffer Pool），来提高数据库的读写性能\n有了缓冲池后：\n读取数据时，如果数据存在于Buffer Pool中，客户端就会直接读取Buffer Pool中的数据，否则再去磁盘中读取 当修改数据时，首先修改Buffer Pool中数据所在的数据页，然后将该页设置为脏页，最后由后台线程将脏页写入到磁盘 Buffer Pool有多大？ Buffer Pool在MySQL启动的时候，向操作系统申请的一片连续的内存空间，默认配置下Buffer Pool只有128MB\n可以通过调整innodb_buffer_pool_size 参数来设置Buffer Pool的大小，一般建议设置为可用物理内存的60%~80%\nBuffer Pool缓存什么？ InnoDB会把存储的数据分为若干个页，以页作为磁盘和内存交互的基本单位，一个页的默认大小为**16kb，**因此Buffer Pool同样需要按页来划分\n在MySQL启动的时候，**InnoDB会为Buffer Pool申请一片连续的内存空间，然后按照默认的16kb的大小划分出一个个的页，Buffer Pool中的页就叫做缓存页。**这些缓存页都是空的，之后随着程序的运行，才会有磁盘上的页被缓存到Buffer Pool中\n所以，MySQL刚启动的时候，其使用的虚拟内存空间很大，而使用到的物理内存空间很小，这时因为这些虚拟内存被访问后，操作系统才会触发缺页中断，接着将虚拟地址和物理地址建立映射关系\nBuffer Pool缓存了以下的：\n索引页 数据页 插入缓存页 Undo页 自适应哈希索引 锁信息 为了更好管理Buffer Pool中的缓存页，InnoDB为每一个缓存页都创建了一个**控制块，**控制块包括缓存页的表空间，页号，缓存页地址，链表节点等，控制块也占据内存空间，它是在Buffer Pool的最前面，接着才是缓存页\n暂时无法在飞书文档外展示此内容\n上面的控制块和缓存页之间的空白空间称为碎片空间\n碎片空间：每一个控制块对应一个缓存页，在分配足够多的控制块和缓存页后，可能剩余的空间不足够一个控制块和缓存页的大小，那么这块空间就不被使用，剩下的这块空间就被称为碎片\n当Buffer Pool的大小设置的刚刚好，就不会产生碎片\n查询一条记录时，InnoDB会把整个页的数据加载到Buffer Pool中，通过索引只能定位到磁盘中的页，而不能定位到页中一条记录。\nmp.weixin.qq.com(从数据页的角度看B+树——InnoDB存储引擎)\n记录是按照行来存储的，但是数据库的读取并不是以行为单位，否则一次读取（一次IO操作）只能处理一行数据，效率会非常低，因此，InnoDB的数据是按照数据页为单位来读写的\n数据页的结构分为7个部分\nFile Header(38) 文件头，表示页的信息 Page Header(56) 页头，表示页的状态信息 infimum+supermun(26) 两个虚拟伪记录，分别表示页中最小记录和最大记录 User Records(unclear) 存储行记录内容 Free Space(unclear) 页中还没被使用的 Page Directory(unclear) 页目录，存储用户记录的相对位置，对记录起索引作用 File Tailer(8) 校验页是否完整 其中，行记录由infimum+supremum 和 User Records构成\n在File Header 中有两个指针，分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向链表\n采用链表结构是让数据页之间不需要物理上的连续，而是逻辑上的连续\n数据页中User Records是怎么组织数据的？ **数据页中的记录按照主键顺序组成单向链表，**单向链表的特点是插入、删除非常方便，但是检索效率不高\n因此，在数据页中有一个页目录（Page Directory），起记录的索引作用，可以快速找到记录\n页目录创建过程如下：\n将所有记录划分为几个组，这些记录包括最小记录和最大记录，但不包括标记已删除的记录\n每个记录组的最后一条记录是组内最大的那条记录，并且最后一条记录的头信息都会存储该组一共多少条记录，作为n_owned字段\n页目录用来存储每组最后一条记录的地址偏移量，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称为槽（slot），每个槽相当于指针指向了不同组的最后一个记录\n页目录就是由多个槽组成，槽相当于分组记录的索引。因为记录是按照主键值大小从小到大排序，所以通过槽查找记录时，可以使用二分查找法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，在遍历槽内的所有记录，找到对应的记录\nInnoDB里的B+树中的每个节点都是一个数据页\nInnoDB对每个分组中的记录条数是有规定的，槽内的记录就有几条：\n第一个分组中的记录只能由1条\n最后一个分组的记录条数范围只能在1-8条之间\n剩下的分组中记录条数范围只能在4-8条之间\n如何管理Buffer Pool？ 空闲页的管理 为了能够快速找到空闲的缓存页，可以使用链表结构，将空闲缓存页的控制块作为链表的节点，这个链表称为Free链表（空闲链表）\nFree链表上除了控制块，还有一个头结点，该头结点包含该链表的头结点地址，尾节点地址，以及当前链表中节点的数量等信息\nFree链表节点是一个个的控制块，而每个控制块包含着对应缓存页的地址，所以相当于Free链表节点都对应一个空闲缓存页\n有了Free链表后，每当需要从磁盘中加载一个页到Buffer Pool中，就从Free链表中取一个空闲的缓存页，并且把该缓存页对应的控制块的信息填上，然后把该缓存页对应的控制块从Free链表中移除\n脏页的管理 Buffer Pool除了提高读性能，还能提高写性能，就是更新数据的时候，不需要每次都写入磁盘，而将Buffer Pool对应的缓存页标记为脏页，然后由后台线程将脏页写入到磁盘\ninnodb设计出了Flush链表，跟Free链表类似，链表的节点是控制块，区别是Flush链表的元素是脏页\n有了Flush链表，后台线程可以遍历Flush链表，将脏页写入磁盘\n如何提高缓存命中率 Buffer Pool的大小是有限的，所以需要使用一些策略，保证常用数据留在Buffer Pool，少用的数据在某个时机可以淘汰掉\n最常见的是LRU算法（Least recently used）\n这个算法的思路是，链表头部的节点是最近使用的，而链表末尾的节点是最久没有使用的，那么当空间不够时，就淘汰最久没有使用的节点\n简短的LRU算法实现思路如下：\n当访问的页在Buffer Pool中，就直接将该页对于的LRU链表节点移动到链表的头部\n当访问的页不在Buffer Pool中，除了把页放入到LRU链表的头部，还要淘汰LRU链表末尾的节点\n至此，Buffer Pool里有三种页和链表来管理数据：\nFree Page（空闲页）：表示此页未被使用，位于Free链表\nClean Page（干净页）：表示此页已经被使用，但是页面未发生修改，位于LRU链表\nDirty Page（脏页）：表示此页已经被修改，其数据和磁盘上的数据已经不一致。当脏页上的数据写入磁盘后，内存数据和磁盘数据一致，那么该页就变成干净页。脏页同时存在于LRU链表和FLUSH链表\n简短LRU算法没有被MySQL使用，因为简短LRU算法无法避免一些两个问题：\n预读失效\nBuffer Pool污染\n怎么解决预读失效而导致缓存命中率减低的问题？ 预读失效：\nMySQL的预读机制。程序有空间局部性，靠近当前被访问数据的数据，在未来大概率被访问\nMySQL在加载数据页的时候，会提前把相邻的数据页一并加载，减少磁盘IO\n但是这些被提前加载进来的数据页，并没有被访问，相当于预读是白做，这个就是预读失效\n如果使用简单的LRU算法，就会把预读页放到LRU链表头部，而当Buffer Pool空间不够，还需要淘汰末尾的页\n这里会出现一个奇怪的问题，预读页可能一直不会被访问到，却会占用LRU链表前排的位置，而末尾淘汰的页可能是频繁访问的页，这样就大大降低了缓存命中率\n避免预读失效带来的影响，最好就是让预读的页停留在Buffer Pool里时间尽可能的短，让真正被访问的页才移动到LRU链表的头部，从而保证真正被读取的热数据留在Buffer Pool里的时间尽可能长\nMySQL做了以下修改：将LRU划分了2个区域：old区域和young区域\nyoung区域在LRU链表的前半部分，old区域则是在后半部分，old区域占整个LRU链表长度比例可以通过innodb_old_blocks_pct 参数来设置，默认是37，代表整个LRU链表中young区域和old区域比例是63:37\n划分两个区域后，预读的页只需要加入到old区域的头部，当页被真正访问到时候，才将页插入到young区域。\nMySQL改进后的LRU算法，通过划分young区域和old区域避免了预读失效带来的影响，但是没有解决Buffer Pool污染的问题\n怎么解决出现Buffer Pool污染而导致缓存命中率减低的问题？ Buffer Pool污染：\n当某个SQL语句扫描了大量的数据，在Buffer Pool空间比较有限的情况下，可能会将Buffer Pool里的所有页都替换出去，导致大量热数据被淘汰，等这些热数据又再被访问的时候，由于缓存未命中，就会产生大量的磁盘IO，MySQL性能就会急剧下降，这个过程为Buffer Pool污染\n像全表扫描的查询，很多缓存页其实只会被访问一次，但是它却因为被访问一次而进入到young区域，从而导致热点数据被替换\n为了解决这个问题，MySQL提高了进入young区域的门槛，这样就能有效保障young区域里的热点数据不会被替换掉\n想要进去young区域条件增加了一个停留在old区域的时间判断\n具体过程如下，在对某个处在old区域的缓存页进行第一次访问时，就在它对应的控制块中记录下来这个访问时间：\n如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该缓存页就不会被从old区域移动到young区域的头部 如果后续的访问时间与第一次访问的时间不在某个时间间隔内，那么该缓存页移动到young区域的头部 这个间隔时间是由innodb_old_blocks_time 控制的，默认是1000ms\n也就是说，只有同时满足被访问与old区域停留时间超过1s两个条件，才会被插入到young区域的头部，这样就解决了Buffer Pool污染问题\n另外，MySQL针对young区域其实做了一个优化，为了防止young区域节点频繁移动到头部，young区域前面1/4被访问不会移动到链表头部，只有后面的3/4被访问了才会\n脏页什么时候会被刷入磁盘？ 引入Buffer Pool后，当修改数据时，首先修改Buffer Pool中数据所在的页，然后将其页设置为脏页，但是磁盘中还是原数据\n因此，脏页需要刷入磁盘，保证缓存和磁盘数据一致，但是若每次修改数据都刷入磁盘，则性能会变差，因此一般都会在一定时机进行批量刷盘\n但是如果脏页在还没来得急刷入磁盘时，MySQL宕机了，数据会丢失吗？\n不会，InnoDB的更新操作采用的是Write Ahead Log策略，即先写日志，在写入磁盘，通过redo log日志让MySQL拥有崩溃恢复能力\n下面几种情况会触发脏页的刷新：\n当redo log日志满了的情况下，会主动触发脏页刷新到磁盘 Buffer Pool空间不足时，需要将一部分数据页淘汰掉，如果淘汰的脏页，需要先将脏页同步到磁盘 MySQL认为空闲时，后台线程会定期将适量的脏页刷入到磁盘 MySQL正常关闭时，会把所有的脏页刷入磁盘 ","date":"2025-07-03","id":4,"permalink":"/notes/database/mysql/mysql-buffer-pool/","summary":"MySQL 缓存池","tags":"MySQL","title":"Mysql Buffer Pool(bate)"},{"content":" MySQL 架构- server \u0026amp;\u0026amp; storage-engine\n6.1 SQL执行过程 推荐阅读 小林coding/mysql\nMySQL架构分为两层：server层和存储引擎层\nServer层负责建立连接、分析和执行SQL MySQL大多数核心功能模块都在这里：连接器、查询缓存、解析器、预处理器、优化器、执行器等 还有所有的内置函数 所有跨存储引擎的功能 存储引擎层负责数据的存储和提取 支持InnoDB、MyISAM、Memory等多个存储引擎 6.1.1 连接器 MySQL是基于TCP协议进行传输的，所以在连接MySQL的时候需要先进行TCP三次握手，在命令行使用命令进行连接\nmysql -h $ip -u$user -p 用户通过用户密码成功连接后，连接器会获取用户的权限，然后保存起来，在后续的此连接的任何操作，都会基于连接开始的时候读取到的权限逻辑进行判断\n建立连接后，即使修改了该用户的权限，也不影响已连接的权限。只有新建的连接才会有新的权限设置\n6.1.1.1 查看MySQL服务的客户端连接 可以执行show processlist 命令进行查看\n6.1.1.2 空闲连接会一直占着 不会，MySQL定义了空闲连接的最大空闲时长，由wait_timeout 参数控制，默认值是8小时，超过这个时间，连接器就会把这个连接断开\n使用命令可以查看该值\nshow variables like \u0026#39;wait_timeout\u0026#39;; 可以手动断开空闲的连接，使用的是\nkill connection + id 当空闲的连接被服务端主动断开后，这个客户端并不会马上知道，等到客户端在发起下一个请求时，才会收到报错\n“ERROR 2013 (HY000): Lost connection to MySQL server during query”\n6.1.1.3 MySQL的连接限制 MySQL服务支持的最大连接数由max_connections 参数控制\nshow variables like \u0026#39;max_connections\u0026#39;; MySQL的连接跟HTTP一样，有短连接和长连接的概念\n// 短连接 连接 mysql 服务（TCP 三次握手） 执行sql 断开 mysql 服务（TCP 四次挥手） // 长连接 连接 mysql 服务（TCP 三次握手） 执行sql 执行sql 执行sql .... 断开 mysql 服务（TCP 四次挥手） 一般推荐长连接，但是使用长连接可能会占用内存增多，因为_MySQL在执行查询过程中临时使用内存管理连接对象__，_只有在连接断开的时候才会释放\n6.1.1.4 怎么解决长连接占用内存的问题 两个解决方案：\n定期断开长连接\n客户端主动重置连接\nMySQL在5.7版本实现了mysql_reset_connection()函数的接口，可以使得客户端执行一个很大的操作后，在代码里调用该函数，来进行重置连接，达到释放内存的效果\n与客户端进行 TCP 三次握手建立连接；\n校验客户端的用户名和密码，如果用户名或密码不对，则会报错；\n如果用户名和密码都对了，会读取该用户的权限，然后后面的权限逻辑判断都基于此时读取到的权限；\n6.1.2 查询缓存 连接器完成连接后，服务端收到SQL语句，就会解析出SQL语句是什么类型的语句\n如果是SELECT语句，MYSQL会先去查询缓存（Query Cache）查找缓存数据，这个查询缓存是以key-value 形式保存在内存中，key为SQL查询语句，value为SQL语句查询的结果\n如果缓存命中，就会直接发送value给客户端，否则就继续往下执行\n在MySQL8.0版本，这个查询缓存被删除了，因为这个查询缓存的命中率很低，因为只要有一个表有更新操作，那么这个表的查询缓存就会被清空，如果刚缓存了一个查询结果很大的数据，还没有使用，刚好这个表有更新操作，查询缓存就被清空了，相当于缓存浪费了\n这里说的查询缓存是 server 层的，也就是 MySQL 8.0 版本移除的是 server 层的查询缓存，并不是 Innodb 存储引擎中的 buffer pool\n6.1.3 解析SQL 解析器会做下面两件事\n词法解析：识别关键字\n语法解析：根据词法解析的结果，根据语法规则，构建出SQL语法树\n如果我们输入的SQL语句语法不对，就会在解析器这个阶段报错\n注意：表不存在或者字段不存在，并不在解析器里识别。解析器只负责检查语法和构建语法树，但不会去查表或者字段存不存在\n6.1.4 执行SQL 经过解析器后，进入执行SQL查询语法的流程，主要可分为下面三个阶段\nprepare阶段，预处理阶段\noptimize阶段，优化阶段\nexecute阶段，执行阶段\n6.1.4.1 预处理器 预处理阶段做以下的事：\n检查SQL查询语句中的表或者字段是否存在\n将*扩展为表上所有列\n6.1.4.2 优化器 预处理阶段后，需要为SQL语句制定一个执行计划，就交由优化器完成\n优化器主要负责将SQL查询语句的执行方案确定下来，决定使用哪个索引\n在查询语句前加个explain命令，就会输出这条SQL语句的执行计划\n6.1.4.3 执行器 执行器：开始真正执行语句。在执行过程中，执行器就会和存储引擎交互，过程如下\n主键索引查询\n全表扫描\n索引下推\n6.1.4.3.1 主键索引查询 在SQL语句中查询条件使用主键索引，访问类型为const，那么执行器与存储引擎执行流程大致如下\n执行器第一次查询，调用read_first_record函数指针指向函数，访问类型为const，指向InnoDB引擎索引查询的接口，让存储引擎定位符合条件的记录\n存储引擎通过主键索引的B+树结构定位到符合条件的记录，如果记录不存在，就会向执行器上报记录找不到的错误，查询结束；如果记录存在，则返回记录给执行器\n执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则发送给客户端，不符合则跳过该记录\n执行器查询的过程是一个while循环，所以会在查询一次，此时调用read_record函数指针指向的函数，因为优化器选择的访问类型是const，这个函数指针指向一个永远返回-1的函数，所以当调用函数的时候，执行器退出循环，查询结束\n6.1.4.3.2 全表扫描 全表查询是没有用到索引，所以优化器决定选用访问类型为ALL\n执行器第一次查询，调用read_first_record函数指针指向函数，访问类型为const，指向InnoDB引擎全扫描的接口，让存储引擎定位符合条件的记录\n执行器会判断读到的记录是不是符合条件，不是则跳过；是则将记录发送给客户（Server 层每从存储引擎读到一条记录就会发送给客户端，之所以客户端显示的时候是直接显示所有记录的，是因为客户端是等查询语句查询完成后，才会显示出所有的记录）\n执行器查询的过程是一个 while 循环，所以还会再查一次，会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 all，read_record 函数指针指向的还是 InnoDB 引擎全扫描的接口，所以接着向存储引擎层要求继续读刚才那条记录的下一条记录，存储引擎把下一条记录取出后就将其返回给执行器（Server层），执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端；\n一直重复上述过程，直到存储引擎把表中的所有记录读完，然后向执行器（Server层） 返回了读取完毕的信息；\n执行器收到存储引擎报告的查询完毕的信息，退出循环，停止查询。\n6.1.4.3.3 索引下推 索引下推能够减少二级索引在查询时的回表操作，提高查询的效率（它是将server层部分负责的事，交由存储引擎层去处理）\n总结 执行一条 SQL 查询语句，期间发生了什么？\n连接器：建立连接，管理连接、校验用户身份；\n查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0 已删除该模块；\n解析 SQL，通过解析器对 SQL 查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型；\n执行 SQL：执行 SQL 共有三个阶段：\n预处理阶段：检查表或字段是否存在；将 select * 中的 * 符号扩展为表上的所有列。\n优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划；\n执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端；\n6.2 MySQL存储一行记录 总结 MySQL的NULL值是怎么存放的？ MySQL的Compact行格式中会使用NULL值列表来标记NULL的列，NULL值并不会存储在行格式中的真实数据\nNULL值列表会占用1字节空间，当表中所有字段都定义成NOT NULL，行格式就不会有NULL值列表，可以节省1字节空间\n6.? 引擎分类 https://juejin.cn/post/7160557698642083847\n","date":"2025-07-03","id":5,"permalink":"/notes/database/mysql/mysql-arch/","summary":"MySQL 架构- server \u0026amp;\u0026amp; storage-engine","tags":"MySQL","title":"Mysql Arch(bate)"},{"content":" Git 仓库 README.md 规范\n# 项目名称 \u0026lt;!-- 写一段简短的话描述项目 --\u0026gt; ## 功能特性 \u0026lt;!-- 描述该项目的核心功能点 --\u0026gt; ## 软件架构(可选) \u0026lt;!-- 可以描述下项目的架构 --\u0026gt; ## 快速开始 ### 依赖检查 \u0026lt;!-- 描述该项目的依赖，比如依赖的包、工具或者其他任何依赖项 --\u0026gt; ### 构建 \u0026lt;!-- 描述如何构建该项目 --\u0026gt; ### 运行 \u0026lt;!-- 描述如何运行该项目 --\u0026gt; ## 使用指南 \u0026lt;!-- 描述如何使用该项目 --\u0026gt; ## 如何贡献 \u0026lt;!-- 告诉其他开发者如果给该项目贡献源码 --\u0026gt; ## 社区(可选) \u0026lt;!-- 如果有需要可以介绍一些社区相关的内容 --\u0026gt; ## 关于作者 \u0026lt;!-- 这里写上项目作者 --\u0026gt; ## 谁在用(可选) \u0026lt;!-- 可以列出使用本项目的其他有影响力的项目，算是给项目打个广告吧 --\u0026gt; ## 许可证 \u0026lt;!-- 这里链接上该项目的开源许可证 --\u0026gt; ","date":"2025-07-03","id":6,"permalink":"/notes/git/git-repo-readme/","summary":"Git 仓库 README.md 规范","tags":"Git","title":"Git Repo Readme(bate)"},{"content":" 写出符合Angular规范的Git Commit Message\ngit commit 规范 符合Angular规范的Commit Message \u0026lt;type\u0026gt;[(optional scope)]: \u0026lt;description\u0026gt; // 空行 [optional body] // 空行 [optional footers] 分为了Header、Body、footer三个部分\nHeader Header部分只有一行\u0026lt;type\u0026gt;[(optional scope)]: \u0026lt;description\u0026gt;，其中type必选，其它可选\ntype\u0026ndash;\u0026gt;归为两类：\nDevelopment(项目管理类变更，不影响用户和生产环境的代码) Production(影响用户和生产环境的代码) 类型 类别 说明 feat Production 新增功能 fix Production 修复缺陷 perf Production 提高代码性能的变更 style Development 代码格式类的变更，例如使用gofmt格式化代码 refactor Production 其他代码类的变更，例如 简化代码、重命名变量、删除冗余代码等等 test Development 新增测试用例或更新现有的测试用例 ci Development 持续基础和部署相关的改动，例如修改Jenkins、GitLab CI等Ci配置文件或者更新系统单元文件 docs Development 文档类的更新，包括修改用户文档、开发文档 chore Development 其他类型，例如构建流程、依赖管理或者复制工具的变动 scope\u0026ndash;\u0026gt;不设置太具体的值，说明commit的影响范围 description\u0026ndash;\u0026gt;对commit的简短描述，以动词开头\nBody Body对Commit Message的高度概况，方便查看具体做了什么变更\nFooter Footer部分不是必选，可根据需要选择，主要用来说什么本次commit导致的后果，通常用来说明不兼容的改动或者关闭的issue\nBREAKING CHANGE: \u0026lt;breaking change summary\u0026gt; // 空行 \u0026lt;breaking change description + migration instructions\u0026gt; // 空行 // 空行 Fixes(Closes) #\u0026lt;issue number\u0026gt; Revert Commit 特殊的Commit Message。还原了先前的commit，则以revert开头，后面跟还原的commit的Header， 在Body必须写This reverts commit \u0026lt;hash\u0026gt;，其中hash为要还原的commit的SHA标识\n使用git commit -a进入交互界面的Commit Message\n","date":"2025-07-03","id":7,"permalink":"/notes/git/git-commit/","summary":"写出符合Angular规范的Git Commit Message","tags":"Git","title":"Git Commit(bate)"},{"content":" git 命令\ngit rebase git rebase的最大作用是重写历史\n使用git rebase -i \u0026lt;commit ID\u0026gt;使用git rebase命令 修改某次 commit 的 message\n命令 目的 p,pick 不对该commit做任何处理 r,reword 保留该commit，但是修改提交信息 e,edit 保留该commit，但是rebase是会暂停，允许你修改这个commit s,squash 保留该commit，但是将当前commit与上一个commit合并 f,fixup 与squash相同，但不会保存当前commit的提交信息 x,exec 执行其他shell命令 d,drop 删除该commit git commit -amend git commit –amend：修改最近一次 commit 的 message\n","date":"2025-07-03","id":8,"permalink":"/notes/git/git-commands/","summary":"git 命令","tags":"Git","title":"Git Commands(bate)"},{"content":" 使用Docker 以及Docker Compose部署Go程序\n部署示例 1.准备代码 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/\u0026#34;, hello) server := \u0026amp;http.Server{ Addr: \u0026#34;:8888\u0026#34;, } fmt.Println(\u0026#34;server startup...\u0026#34;) if err := server.ListenAndServe(); err != nil { fmt.Printf(\u0026#34;server startup failed, err:%v\\n\u0026#34;, err) } } func hello(w http.ResponseWriter, _ *http.Request) { w.Write([]byte(\u0026#34;hello liwenzhou.com!\u0026#34;)) } 这里是简单代码\n2.创建Docker镜像 镜像(image)包含运行应用程序所需的所有东西——代码/二进制文件、运行时、依赖项以及所需的任何其它人间系统对象\n简单讲，镜像是定义应用程序以及运行所需的一切\n3.编写Dockerfile 要创建Docker镜像(image)必须在配置文件中的指定步骤，这个文件默认称为Dockerfile\nFROM golang:alpine # 为我们的镜像设置必要的环境变量 ENV GO111MODULE=on \\ CGO_ENABLED=0 \\ GOOS=linux \\ GOARCH=amd64 # 移动到工作目录：/build WORKDIR /build # 将代码复制到容器中 COPY . . # 将我们的代码编译成二进制可执行文件app RUN go build -o app . # 移动到用于存放生成的二进制文件的 /dist 目录 WORKDIR /dist # 将二进制文件从 /build 目录复制到这里 RUN cp /build/app . # 声明服务端口 EXPOSE 8888 # 启动容器时运行的命令 CMD [\u0026#34;/dist/app\u0026#34;] 4.Dockerfile解析 From 使用了基础镜像 golang:alpine来创建镜像。这个镜像运行的是alpine Linux发行版，该发行版的大小很小并内置了Go。有大量公开可用的Docker镜像，请查看https://hub.docker.com/_/golang\nEnv 用来设置编译阶段需要的环境变量\nWORKDIR,COPY,RUN\nEXPORT,CMD\n声明服务端口，应用程序监听这个端口并通过这个端口对外提供服务。还定义了运行镜像执行的默认执行命令CMD [\u0026quot;/dist/app\u0026quot;]\n构建镜像 在项目目录下面，在终端输入下面的命令创建镜像，并指定镜像名称为go_app\ndocker build . -t go_app 等待构建结束，输出 Successfully\n等输出 Successfully后，此时镜像已经准备好了，但是目前什么项目都没有，需要运行下面的代码来运行镜像。注：运行中的镜像称为镜像\ndocker run -p 8888:8888 go_app 标志位-p来定义端口绑定，由于容器中的应用程序在端口8888上运行，这里绑定的主机端口也是8888。如果要绑定另一个端口，则可以使用 -p $HOST_PORT:8888\n到这里就可以测试我们的程序是否工作正常，打开 http://127.0.0.1:8888 查看事先定义的响应内容。\n分阶段构建示例 Go程序编译之后可得到一个可执行的二进制文件，在最终的镜像中不需要go编译器，也就是说我们只需要一个运行最终二进制文件的容器即可。\nDocker的最佳实践之一是通过仅保留二进制文件来减小镜像大小，为此，我们将使用一种称为多阶段构建的技术\nFROM golang:alpine AS builder # 为我们的镜像设置必要的环境变量 ENV GO111MODULE=on \\ CGO_ENABLED=0 \\ GOOS=linux \\ GOARCH=amd64 # 移动到工作目录：/build WORKDIR /build # 将代码复制到容器中 COPY . . # 将我们的代码编译成二进制可执行文件 app RUN go build -o app . ################### # 接下来创建一个小镜像 ################### FROM scratch # 从builder镜像中把/dist/app 拷贝到当前目录 COPY --from=builder /build/app / # 需要运行的命令 ENTRYPOINT [\u0026#34;/app\u0026#34;] 使用这种技术，我们剥离了使用golang:alpine作为编译镜像来编译得到二进制可执行文件的过程，并基于scratch生成一个简单的、非常小的新镜像。我们将二进制文件从命名为builder的第一个镜像中复制到新创建的scratch镜像中。有关scratch镜像的更多信息，请查看https://hub.docker.com/_/scratch\n附带其他文件的部署示例 web项目(前后端不分离)一般会有静态文件或者配置文件，需要拷贝到最终的镜像文件中\n例如 templates | static | conf 三个文件的内容拷贝到镜像文件中\nFROM golang:alpine AS builder # 为我们的镜像设置必要的环境变量 ENV GO111MODULE=on \\ CGO_ENABLED=0 \\ GOOS=linux \\ GOARCH=amd64 # 移动到工作目录：/build WORKDIR /build # 复制项目中的 go.mod 和 go.sum文件并下载依赖信息 COPY go.mod . COPY go.sum . RUN go mod download # 将代码复制到容器中 COPY . . # 将我们的代码编译成二进制可执行文件 bubble RUN go build -o bubble . ################### # 接下来创建一个小镜像 ################### FROM scratch COPY ./templates /templates COPY ./static /static COPY ./conf /conf # 从builder镜像中把/dist/app 拷贝到当前目录 COPY --from=builder /build/bubble / # 需要运行的命令 ENTRYPOINT [\u0026#34;/bubble\u0026#34;, \u0026#34;conf/config.ini\u0026#34;] Tips： 这里把COPY静态文件的步骤放在上层，把COPY二进制可执行文件放在下层，争取多使用缓存。\n关联其他容器 项目中使用了MySQL，可以选择使用如下命令启动一个MySQL容器，它的别名为mysql8019；root用户的密码为root1234；挂载容器中的/var/lib/mysql到本地的/Users/docker/mysql目录；内部服务端口为3306，映射到外部的13306端口。\ndocker run --name mysql8019 -p 13306:3306 -e MYSQL_ROOT_PASSWORD=root1234 -v /Users/q1mi/docker/mysql:/var/lib/mysql -d mysql:8.0.19 这里需要修改一下我们程序中配置的MySQL的host地址为容器别名，使它们在内部通过别名（此处为mysql8019）联通。\n[mysql] user = root password = root1234 host = mysql8019 port = 3306 db = bubble 修改后记得重新构建bubble_app镜像：\ndocker build . -t bubble_app 我们这里运行bubble_app容器的时候需要使用--link的方式与上面的mysql8019容器关联起来，具体命令如下：\ndocker run --link=mysql8019:mysql8019 -p 8888:8888 bubble_app Docker Compose模式 除了像上面一样使用--link的方式来关联两个容器之外，我们还可以使用Docker Compose来定义和运行多个容器。\nCompose是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，你可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。\n使用Compose基本上是一个三步过程：\n使用Dockerfile定义你的应用环境以便可以在任何地方复制。\n定义组成应用程序的服务，docker-compose.yml 以便它们可以在隔离的环境中一起运行。\n执行 docker-compose up命令来启动并运行整个应用程序。\n我们的项目需要两个容器分别运行mysql和bubble_app，我们编写的docker-compose.yml文件内容如下：\n# yaml 配置 version: \u0026#34;3.7\u0026#34; services: mysql8019: image: \u0026#34;mysql:8.0.19\u0026#34; ports: - \u0026#34;33061:3306\u0026#34; command: \u0026#34;--default-authentication-plugin=mysql_native_password --init-file /data/application/init.sql\u0026#34; environment: MYSQL_ROOT_PASSWORD: \u0026#34;root1234\u0026#34; MYSQL_DATABASE: \u0026#34;bubble\u0026#34; MYSQL_PASSWORD: \u0026#34;root1234\u0026#34; volumes: - ./init.sql:/data/application/init.sql bubble_app: build: . command: sh -c \u0026#34;./wait-for.sh mysql8019:3306 -- ./bubble ./conf/config.ini\u0026#34; depends_on: - mysql8019 ports: - \u0026#34;8888:8888\u0026#34; 这个 Compose 文件定义了两个服务：bubble_app 和 mysql8019。其中：\nbubble_app 使用当前目录下的Dockerfile文件构建镜像，并通过depends_on指定依赖mysql8019服务，声明服务端口8888并绑定对外8888端口。\nmysql8019 mysql8019 服务使用 Docker Hub 的公共 mysql:8.0.19 镜像，内部端口3306，外部端口33061。\n这里需要注意一个问题就是，我们的bubble_app容器需要等待mysql8019容器正常启动之后再尝试启动，因为我们的web程序在启动的时候会初始化MySQL连接。这里共有两个地方要更改，第一个就是我们Dockerfile中要把最后一句注释掉：\n# Dockerfile ... # 需要运行的命令（注释掉这一句，因为需要等MySQL启动之后再启动我们的Web程序） # ENTRYPOINT [\u0026#34;/bubble\u0026#34;, \u0026#34;conf/config.ini\u0026#34;] 第二个地方是在bubble_app下面添加如下命令，使用提前编写的wait-for.sh脚本检测mysql8019:3306正常后再执行后续启动Web应用程序的命令：\ncommand: sh -c \u0026#34;./wait-for.sh mysql8019:3306 -- ./bubble ./conf/config.ini\u0026#34; 当然，因为我们现在要在bubble_app镜像中执行sh命令，所以不能在使用scratch镜像构建了，这里改为使用debian:stretch-slim，同时还要安装wait-for.sh脚本用到的netcat，最后不要忘了把wait-for.sh脚本文件COPY到最终的镜像中，并赋予可执行权限哦。更新后的Dockerfile内容如下：\nFROM golang:alpine AS builder # 为我们的镜像设置必要的环境变量 ENV GO111MODULE=on \\ CGO_ENABLED=0 \\ GOOS=linux \\ GOARCH=amd64 # 移动到工作目录：/build WORKDIR /build # 复制项目中的 go.mod 和 go.sum文件并下载依赖信息 COPY go.mod . COPY go.sum . RUN go mod download # 将代码复制到容器中 COPY . . # 将我们的代码编译成二进制可执行文件 bubble RUN go build -o bubble . ################### # 接下来创建一个小镜像 ################### FROM debian:stretch-slim COPY ./wait-for.sh / COPY ./templates /templates COPY ./static /static COPY ./conf /conf # 从builder镜像中把/dist/app 拷贝到当前目录 COPY --from=builder /build/bubble / RUN set -eux; \\ apt-get update; \\ apt-get install -y \\ --no-install-recommends \\ netcat; \\ chmod 755 wait-for.sh # 需要运行的命令 # ENTRYPOINT [\u0026#34;/bubble\u0026#34;, \u0026#34;conf/config.ini\u0026#34;] 所有的条件都准备就绪后，就可以执行下面的命令跑起来了：\ndocker-compose up ","date":"2025-07-03","id":9,"permalink":"/notes/docker/docker-deploy/","summary":"使用Docker 以及Docker Compose部署Go程序","tags":"docker","title":"Deploy Go applications using docker(bate)"},{"content":" Dockerfile 构建你自己的容器\n学习自\n一篇文章带你吃透 Dockerfile - 掘金 (juejin.cn) Dockerfile reference 全网最详细的Docker-Compose详细教程 - 掘金 (juejin.cn) docker compose 配置文件 .yml 全面指南 - 知乎 (zhihu.com) compose-spec/spec.md at master · compose-spec/compose-spec · GitHub 学习Dockers前期，通过Docker的官方镜像仓库拉取里面的镜像，根据这些镜像创建出容器并运行\n实际上，Docker官方镜像也是通过一定的方式构建出来的，只要弄清其中的逻辑，我们也可以仿照官方镜像的构建过程，构建出自己的镜像\nDockerfile就是这样一个用于描述Docker镜像构建过程的文本文件，dockerfile可以包含多条构建指令，以及相关的描述\n1.什么是容器 容器是计算机上的沙盒进程，与主机上的其它进程隔离，这种隔离利用了内核命名空间和cgroups。简而言之容器是：\n是image的可运行实例\n可以在本地计算机、虚拟机上运行或部署到云中\n是可移植的\n与其它容器隔离，并运行自己的软件，二进制文件和配置\n2.什么是容器映射 当容器运行时，它使用了隔离的文件系统。这个自定义的文件系统由容器映像container image提供。因为image包含了容器的问价系统，使用image必须包含所有的运行应用程序所必须的所有东西——依赖项、配置、脚本、二进制文件等等。\n沙盒进程是指在计算机系统中，为了保障安全和隔离性而采用的一种技术，将应用程序运行在一个受限制的环境中，限制它们能访问的资源和操作范围，从而避免恶意程序和授权程序对系统的破坏\n3.容器是怎么运行的 当一个容器运行时，它为其文件系统使用来image的各个层。每个容器都有自己的命名空间来创建/更新/删除文件。在另一个容器中不会看到任何更改，即使它们使用相同的image\n4.容器卷[container volumes] 每个容器启动时都是从容器的定义开始的。在容器中可以创建、更新和删除文件，但当容器被删除时，这些改变将回丢失，所有更变都被隔离在各个容器中\n卷：提供了将容器的特定文件系统路径链路到主机的能力。如果在主机上的某个文件被挂载，那么当容器中该文件路径下的文件发送更改时，我们在主机上同样也可以看到更改。同样的，启动另一个挂载了同一个文件目录的容器，它也可以访问到相同的文件\n镜像构建原理 1.Docker架构模式 docker使用了client/server的架构模式。构建镜像时，用户在dockers client输入构建命令。docker引擎以 REST API的形式，像 docker daemon发送构建请求，如何dockers daemon就根据构建请求的内容，开始镜像构建的工作，并向Client持续放回构建过程的信息。\n2.镜像分层模型 docker镜像是用于创建容器的只读模板，是通过 Dockerfile中定义的指令构建而成的，构建结束后，会在原有的镜像层上生成一个新的镜像层，如下所示\n在 tomcat 镜像创建一个容器后，会在tomcat镜像之上新创建一个可写的容器层，在容器中写文件时，会保存到这个容器层中\n3.基础镜像与父级镜像 用于构建基础镜像的 Dockerfile 不指定父级镜像，Docker约定使用如下形式基础镜像\nFROM scratch 这里的 scratch是一个空镜像，可以从零开始构建镜像，常用来构建最小镜像，如busybox，debian，alpine等镜像，省去很多linux命令，因此很小。一般，不需要自己去构建基础镜像。\n构建自定义镜像时，通过FROM指定使用说明父级镜像。例如，官方的tomcat命令没有yum，vim等命令，但是我们可以将tomcat镜像作为父级镜像，然后安装想要的命令，这样在容器中就可以使用了。\n4.构建上下文 / build context Client 向 Docker daemon 发送的构架请求包含两部分，第一部分是 Dockerfile文件，第二部分是构建上下文\n构建上下文是一些文件集合，这些文件可以是指定路径下的文件，也可以是远程资源中指定路径下的文件，在构建过程中，Docker daemon 可以访问这些文件，并执行相应的操作[理解：访问配置文件]\n路径上下文 构建命令中指定具体路径，该路径下的所有文件即为构建上下文，这些文件被打包送给Docker daemon中，然后被解压\n假使一个项目的文件结构如下\ndemo |--Dockerfile |--src |--test |--node_modules 在项目根目录执行下面的构建命令\ndocker build -t img-tag . 构建请求的第一部分是Dockerfile，这个文件在当前目录下，文件是默认名称，因此省略，\n相当于默认加上了 -f Dockerfile, 该Dockerfile内容如下\nFROM busybox WORKDIR /src COPY src . 构建请求的第二部分是 .这个点代表当前，此时当前目录就是此次的构建的上下文，Docker引擎会整理该目录下的所有文件，把不被 .dockerignore中的规则所的文件都发送到Docker daemon中，如下所示\n如果此时位于项目根目录的上一级目录，构建命令如下\ndocker build -t img-tag -f ./demo/Dockerfile ./demo/ URL上下文 Docker 还支持利用远程仓库URL构建镜像，此时指定的远程仓库目录就充当了构建上下文\ndocker build https://gitee.com:user/my-repo.git#master:docker 以上构建命令指定了一个 Gitee 项目的 master 分支，冒号（:）之前是 Git 检出的目标 URL, 冒号之后的 docker 是远程仓库根目录下的一个子目录，此时这个子目录就是构建上下文\nDocker client 执行构建命令时，Docker 引擎首先会将远程仓库的 master 分支拉取到本地的一个临时目录中，然后将其中的 docker 目录下的文件作为构建上下文发送到 Docker daemon 中。拉取远程文件之后，又回到了路径上下文的步骤，如下图所示\n省略上下文 如果 Dockerfile 中的指令不需要对任何文件进行操作，可以省略构建上下文，此时不会向 Docker daemon 发送额外的文件，这可以提高构建速度\ndocker build -t my-hello-world:latest -\u0026lt;\u0026lt;EOF FROM busybox RUN echo \u0026#34;hello world\u0026#34; EOF 5.构建缓存 迭代过程中，Dockerfile对于的资源会被经常修改，因此需要频繁重新构建镜像，Docker为了提高构建速度，设计了多种优化方案，其中最重要的是构建缓存\n示例：说明构建缓存是如何工作的，Dockerfile如下\n# syntax=docker/dockerfile:1 FROM ubuntu:latest RUN apt-get update \u0026amp;\u0026amp; apt-get install -y build-essentials COPY main.c Makefile /src/ WORKDIR /src/ RUN make build 镜像构建过中，dockerfile 中的指令会从上往下执行，每一个构建步骤的结果都会被缓存起来，例如\n此时再次构建，会直接使用缓存中的结果(Using cache)\n这里假设修改了main.c 中的代码，再次构建时，从 COPY main Makefile /src/这条指令开始，后续构建缓存都会失效，如下图所示\n如果不想使用构建缓存，执行构建命令时，可以传入 --no-cahe\n6.镜像构建过程 Docker Client 执行构建命令后，会经过以下步骤构建出最终镜像\n确定构建上下文，如果构建上下文中有 .dockerignore 文件，解析该文件的匹配规则，将构建上下文中被匹配的文件资源排除\n将 Dockerfile 和构建上下文发送给 Docker daemon\nDocker daemon 收到构建请求。以下的步骤都由 Docker daemon 完成，省略主语\n逐条校验 Dockerfile 中的指令是否合法，如果不合法，立即结束构建。这一步可以确定一共有多少个构建步骤，便于后续分步构建时显示当前步骤，如 Step 1/2\n逐条执行 Dockerfile 中的指令，每条指令都新创建一层。会生成临时 container 用于执行命令，该步骤结束后删除临时容器\n生成最终镜像\n.dockerignore 这个文件需要遵循一定的语法规则\n以 # 开头的行是备注，不会被解析为匹配规则\n支持 ? 通配符，匹配单个字符\n支持 * 通配符，匹配多个字符，只能匹配单级目录\n支持 ** 通配符，可匹配多级目录\n支持 ! 匹配符，声明某些文件资源不需要被排除\n可以用 .dockerignore 排除 Dockerfile 和 .dockerignore 文件。Docker Client 仍然会将这两个文件发送到 Docker daemon，因为 Docker 底层需要。但 ADD 和 COPY 指令就无法操作这两个文件了\n示例：\n# this is a .dockerignore demo */demo* */*/demo* demo? **/mydemo* Dockerfile Dockerfile时一个用于描述Docekr镜像构建过程的文本文件，包含多条构建指令，以及相关的描述\nDockerfile的构建指令需要遵循如下的语法\n# Comment INSTRUCTION arguments 以 #开头的行绝大部分是注释，还有一小部分是解析器指令\n构建指令分两个部分，第一部分是指令，第二部分是指令参数。\n1.解析器指令 / parse directive 解析器指令是以 #开始，用来提示解释器对 Dockerfile进行特殊处理，构建过程中它不会增加镜像层，也不会出现在构建过程\n解析器指令是可选的\n# directive=value # 解析器指令需要在空行，注释，构建指令之前 注意事项\n同一解析器指令不能重复\n不区分大小写，按照惯例，推荐小写\n空行、注释、构建指令之后，Docker 不再查找解析器指令，都当成注释\n按照惯例，解析器指令位于 Dockerfile 的第一行，在后面添加空行\n行内的空格被忽略，不支持跨行\nDocker 目前支持两种解析器指令\nsyntax\nescape\nsyntax 解析器指令，只有使用 BuildKit 作为构建器时才生效\nescape 解析器指令，用于指定在 Dockerfile 中使用转义字符\n在 Dockerfile 中，escape 默认为 \\\n# escape=\\ 复制代码 但 Windows 系统中的 \\ 是路径分隔符，推荐将 escape 替换为 `，这和 PowerShell 是一致的\n# escape=` 2.常见指令解析 序号 指令名 功能描述 1 FROM 指定基础镜像或者父级镜像 2 LABEL 为镜像添加元数据 3 ENV 设置环境变量 4 WORKDIR 指定后续指令的工作目录，类似于Linux中的cd命令 5 USER 指定当前构建阶段以及容器运行时的默认用户，以及可选的用户组 6 VOLUME 创建具有指定名称的挂载数据卷，用于数据持久化 7 ADD 将构建上下文中指定目录下的文件复制到镜像文件按系统的指定位置 8 COPY 功能与语法与ADD类似，但是不会自动解压文件，也不能访问网络资源 9 EXPOSE 约定容器运行时监听的端口，通常用于容器与外界之间的通信 10 RUN 用于构建镜像过程中执行目录 11 CMD 构建镜像成功后，所创建的容器启动时执行的命令，常与ENTRYPOINT结合使用 12 ENTRYPOINT 用于配置容器以可执行的方式运行，常与CMD结合使用 FROM\n指定基础镜像或父级镜像\nFORM [--platform=\u0026lt;platform\u0026gt;] \u0026lt;image\u0026gt; [AS \u0026lt;name\u0026gt;] FORM [--platform=\u0026lt;platform\u0026gt;] \u0026lt;image\u0026gt;[:\u0026lt;tag\u0026gt;] [AS \u0026lt;name\u0026gt;] FORM [--platform=\u0026lt;platform\u0026gt;] \u0026lt;image\u0026gt;[@\u0026lt;digest\u0026gt;] [AS \u0026lt;name\u0026gt;] tag或digest是可选项，默认为latest版本为基础镜像\n如果不以任何镜像为基础，使用：FORM scratch.scratch是一个空镜像，用于构建最小镜像\nLABEL\n为镜像添加元数据\nLABEL \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;... 示例: LABEL auth=\u0026#34;ch\u0026#34; \\ version=\u0026#34;1.0.0\u0026#34; \\ decription=\u0026#34;Dockerfile\u0026#34; 使用LABEL定义键值对结构的元数据，一个LABEL可指定多个元数据\n定义元数据值时，尽量使用双引号\n当前镜像可以继承继承镜像或者父级镜像中的元数据时，可以覆盖\n可使用以下命令查看元数据\ndocker image inspect -f=\u0026#39;{{json .ContainerConfig.Labels}}\u0026#39; my-image ENV\n设置环境变量\nENV \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;... ENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; 当前镜像可以继承基础镜像或者父级镜像中的环境变量，也可以覆盖\n使用ENV指定定义的环境变量，最终会持久化到容器中\n运行容器时，可以通过--env =或者-e =覆盖镜像定义中的环境变量\n对只使用在镜像构建过程中的变量，推荐使用ARG，或者内环境变量，这样不会被持久化到最终的镜像中\n内环境变量示例：RUN TEMP=\u0026quot;no persisit\u0026quot;\n查看最终镜像中的环境变量 docker image inspect -f=\u0026#39;{{json .ContainerConfig.Env}}\u0026#39; my-image WORKDIR\n指定后续指令的工作目录，类似linux中的cd命令\nWORKDIR /path/to/workdir 使用Dockerfile中设置的环境变量\nENV DIR_PATH=/demo WORKDIR $DIR_PATH/$DIR_NAME RUN pwd 构建镜像时，pwd 的输出结果是 /demo，因为 $DIR_NAME 未显示指定，直接忽略\n默认的工作目录是/\n可以使用Dockerfile中显示指定的环境变量，包括父级镜像中的环境变量\n父级镜像可能设置工作目录，最佳实践是显示设置当前镜像的工作目录\nUSER\n指定当前构建阶段以及容器运行时的默认用户，以及可选的用户组\nUSER \u0026lt;user\u0026gt;[:\u0026lt;group\u0026gt;] USER \u0026lt;user\u0026gt;[:\u0026lt;GID\u0026gt;] USER \u0026lt;UID\u0026gt;[:\u0026lt;group\u0026gt;] USER \u0026lt;UID\u0026gt;[:\u0026lt;GID\u0026gt;] 使用USER指定用户后，Dockerfile中构建镜像的RUN,CMD,ENTRYPOINT指令都会使用该用户，同时这个用户也是容器运行时的默认用户\n不指定用户组，使用默认用户组root\n运行容器时，可以使用-u参数覆盖Dockerfile中默认的用户\nVOLUME\n创建具有指定名称的挂载数据卷，用于数据持久化\nVOLUME [\u0026#34;volume1\u0026#34;,\u0026#34;volume2\u0026#34;,...] VOLUME volume1 volume2 ... 数据卷的特征以及作用：\n数据持久化，避免容器重启后丢失重要数据\n修改数据卷时不会对容器产生影响，防止容器不断膨胀\n有利于多个容器共享数据\nADD\n将构建上下文中指定目录下的文件**(src)复制到镜像文件系统的指定位置(dest)**\nADD [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;][--checksum=\u0026lt;checksum\u0026gt;]\u0026lt;src\u0026gt;... \u0026lt;dest\u0026gt; ADD [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;][\u0026#34;\u0026lt;src\u0026gt;\u0026#34;, ...\u0026#34;\u0026lt;dest\u0026gt;\u0026#34;] ADD \u0026lt;git ref\u0026gt; \u0026lt;dir\u0026gt; 如果ADD指令对应的src资源有变更，Dockerfile中这条指令后的构建缓存都会失效\nDockerfile中--chown特性只有在linux下才有效，windows是无效的\nsrc支持通配符\ndest必须是文件夹，用以存放文件\n如果src是压缩资源，将会被解压为一个文件\n如果 src 是远程 URL, 并且 dest 不以 / 结尾，Docker 从 URL 下载文件，存到 dest 中\n如果 src 是远程 URL，URL 中含有非空路径，并且 dest 以 / 结尾，Docker 会推断文件名，根据 URL 中的路径，在目标位置创建相同路径，将下载的文件放入其中\ndest 可以是镜像文件系统下的绝对路径，或者是 WORKDIR 下的相对路径\n如果 dest 不是以 / 结尾，Docker 会把它当成普通文件，src 中的内容会被写入这个文件中\n如果目标位置下的某些目录不存在，会自动创建\nADD 添加网络资源时不支持身份认证，可以使用 RUN wget 或者 RUN curl 实现这个功能\nCOPY\n功能与ADD类似，但是不会自动解压文件，也不能访问网络资源\nCOPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] \u0026lt;src\u0026gt;... \u0026lt;dest\u0026gt; COPY [--chown=\u0026lt;user\u0026gt;:\u0026lt;group\u0026gt;] [\u0026#34;\u0026lt;src\u0026gt;\u0026#34;,... \u0026#34;\u0026lt;dest\u0026gt;\u0026#34;] EXPOSE\n约定容器运行时监听的端口，通常用于容器与外界之间的通信\nEXPOSE \u0026lt;port\u0026gt; [\u0026lt;port\u0026gt;/\u0026lt;protocol\u0026gt;...] 支持 TCP 或者 UDP 协议，如果不显式指定协议，默认使用 TCP 协议\n需要同时以 TCP 和 UDP 协议的方式暴露同一个端口时，需要分别指定\nEXPOSE 并不会真正将端口发布到宿主机，而是作为一种约定，让镜像使用者在运行容器时，用 -p 分别发布约定端口，或者 -P 发布所有约定端口\n如果没有暴露端口，运行容器是也可以通过 -p 的方式映射端口\nRUN\n用于构建镜像过程中执行命令，有两种执行方式\n第一种，以shell方式运行\nRUN \u0026lt;command\u0026gt; RUN echo \u0026#34;Hello Dockerfile\u0026#34; 第二种，以exec的方式运行\nRUN [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;...] CMD\n构建镜像成功后，所创建的容器启动时执行的命令\nCMD command param1 param2 #shell CMD [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] #exec CMD [\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] #作为ENTRYPOINT的默认参数，是exec方式的特殊形式 Docker种只有一条CMD指令生效，在多条CMD指令存在的情况下，只有最后一条生效\n虽然Dockerfile中只有一条CMD生效，但每一条CMD指令会新增一个镜像层，所有推荐只定义一条CMD指令，使用\u0026amp;\u0026amp;连接多个指令\nexec方式是通过JSON数组的方式进行解析的，因此需要双引号\n与RUN指令不同，RUN指令是在构建指令的过程中执行，CMD命令是在容器启动时执行\ndocker run后的命令行参数会覆盖CMD中的命令\nENTRYPOINT\n用于配置容器以可执行的方式运行。有两种形式\nENTRYPOINT [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] #推荐 ENTRYPOINT command param1 param2 Dockerfile中只有最后一条ENTRYPOINT指令生效\n运行容器时，docker run \u0026ndash;entrypoint 覆盖该指令\nshell 形式的 ENTRYPOINT 会使 CMD 命令 和 docker run\n中的命令行参数失效。它有一个缺点，ENTRYPOINT 命令将作为 /bin/sh -c 的子命令，不会传递信号。比如，停止容器时，容器内接收不到 SIGTERM 信号，这并不是预期的效果，可以在命令前添加 exec 来解决，如 ENTRYPOINT exec top -b\n指定 ENTRYPOINT 后，CMD 的内容将作为默认参数传给 ENTRYPOINT 指令，形如\n如果 CMD 是在基础镜像中定义的，当前镜像定义的 ENTRYPOINT 会将 CMD 的值重置为空值，这种情况下，需要重新定义 CMD\nDocker-Compose docker-compose通过一个声明式的配置文件描述整个应用，从而使用一条命令即可完成部署\ndocker-compose同使用YAML文件来定义多级服务，在使用时默认使用文件名docker-compose.yml，也可以在docker compose运行时使用-f参数来指定具体文件\ncompose的优点\n在单主机上建立多个隔离环境，Compose使用项目名称将环境彼此隔离，可以在多个不同的上下文中使用此项目名称\n创建容器时保留卷数据\n仅重新创建以更改的容器，当服务没有更改时，Compose会使用现有的容器\n变量在环境之间组合重复使用\n多个配置文件\n可以为用一个项目配置多个compose文件，使用多个compose文件能够针对不同的环境或者不同的工作流自定义compose应用程序\n默认情况下，compose读取两个文件，docker-compose.yml和一个可选docker-compose.override.yml文件\n如果在两个文件中都定义了服务，compose会使用override进行合并配置\n当配置文件的名称非默认情况时，可以使用-f指定Compose文件\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d yaml文件级\nDocker compose的YAML文件包含有4个一级key:version,services,networks,volumes\nversion:指定版本信息，通常位于文件的第一行。其定义了Compose文件格式的版本。\nservices:用于定义不用的应用服务。Docker Compose会将每个服务部署在各种的容器中。\nnetworks:用于指引Docker创建新的网络。默认情况下，Docker Compose会创建bridge网络，这个是一个单主机网络，只能实现同一主机上容器的连接。可以使用driver属性指定不同的网络类型\nvolumes用于指引Docker来创建新的卷\ndocker-compose.yml的具体配置： 1.build 指定构建镜像的dockerfile的上下文路径，或者详细配置文件\nversion: \u0026#34;3.9\u0026#34; services: webapp: build: ./dir 或者更详细的写法\nversion: \u0026#34;3.9\u0026#34; services: webapp: build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 context 上下文路径，可以是文件路径，也可以是到链接到 git 仓库的 url。当是相对路径时，它被解释为相对于 Compose 文件的位置。\ndockerfile 指定构建镜像的 Dockerfile 文件名\nargs 构建参数，只能在构建过程中访问的环境变量\ncache_from 缓存解析镜像列表\nlabels 设置构建镜像的元数据\nnetwork 设置网络容器连接，none 表示在构建期间禁用网络\nshm_size 设置/dev/shm此构建容器的分区大小\ntarget 多阶段构建，可以指定构建哪一层\n2.network \u0026hellip;累了，下次再写\nversion: \u0026#39;3.9\u0026#39; services: mysql: build: context: ./mysql environment: MYSQL_ROOT_PASSWORD: admin restart: always container_name: mysql volumes: - /data/edu-bom/mysql/test:/var/lib/mysql image: mysql/mysql:5.7 ports: - 3306:3306 networks: net: eureka: build: context: ./edu-eureka-boot restart: always ports: - 8761:8761 container_name: edu-eureka-boot hostname: edu-eureka-boot image: edu/edu-eureka-boot:1.0 depends_on: - mysql networks: net: networks: net: volumes: vol: docker compose常用命令\n构建并启动服务——docker-compose up -d\n停止运行并删除服务——docker-compose down\n列出所有运行容器——docker-compose ps\n查看服务日志——docker-compose logs\n构建或重建——docker-compose build\n启动服务——docker-compose start\n停止运行中的服务——docker-compose stop\n重启服务——docker-compose restart\n","date":"2025-07-03","id":10,"permalink":"/notes/docker/dockerfile/","summary":"Dockerfile 构建你自己的容器","tags":"docker","title":"Dockerfile(bate)"},{"content":" Docker 一文\nhttps://yeasy.gitbook.io/docker_practice/\n一、基本概念 镜像(Image) 容器(Container) 仓库(Repository) 理解以上三个概念，就能理解docker的生命周期\n1.镜像 Docker镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件，以及一些运行时所需的配置参数。镜像不包含任何动态数据，其内容在插件之后也不会被改变\n分层存储，镜像采用了分层存储的架构，由一组文件系统组成的（多层文件系统联合组成）。在构建镜像时，会一层一层构建，后一层依赖于上一层，后一层上的任何改变都只会发生在本层，不会干涉到上一层。因此构建镜像的时候，需要对每层需要添加的东西尽量加最少最有必要的东西，减少额外的东西\n分层存储的特征还使得镜像的复用，定制更为容易\n2.容器 容器是镜像运行时的实体，可以被创建、启动、停止、删除暂停等\n镜像(Image)和容器(container)的关系，就像是面向对象程序设计中的类 和实例 一样\n容器的实质是进程，运行于属于自己的独立的命名空间。因此容器可以拥有自己的root 文件系统，网络配置、进程空间等，运行在一个隔离的环境。这样的隔离特性，使得容器封装的应用比直接在宿主运行更加安全\n容器也是分层存储，是以镜像为基础层，在其上创建一个当前容器的存储层，这个层是为容器运行时进行读写而准备的，称为容器存储层\n容器存储层的生命周期跟容器一样，当容器消亡时，容器存储层也随之消亡，因此任何保存于容器存储层的信息都会随着容器的删除而丢失\nDokcer最佳实践的要求，容器不应该向其存储层写入任何数据，容器存储层保存无状态化，所有的文件写入操作，都应该使用数据卷、或者绑定宿主目录\n数据卷独立于容器，使用容器卷，容器的删除或者重写运行之后，数据都不会丢失\n3.仓库 Docker Register：提供一个集中的存储、分发镜像的服务\n一个Docker Register可以包含多个**仓库（Repository）;每个仓库可以包含多个标签（Tag）,**每个标签对应一个镜像\n可以通过\u0026lt;Repository Name\u0026gt;:\u0026lt;Tag Name\u0026gt; 的格式来指定具体的软件是那个版本的镜像\n仓库名以两段路径形式出现，比如jwilder/nginx-proxy 前者是Docker Registry多用户环境下的用户名，后者是对应的软件名\nDocker Registry 公开服务 Docker Registry公开服务是开放给用户使用、允许用户管理镜像的Registry服务。\n最常见的是Docker Registry公开服务是官方的hub.docker.com，也是默认的Registry\n也可以使用国内的镜像网站\n私有Docker Registry 用户可以在本地搭建私有的Docker Registry。Docker提供了Docker Registry镜像，可以直接使用搭建私有Registry服务\n二、镜像 1.获取镜像 从Docker镜像仓库获取镜像的命令是 docker pull\ndocker pull [选项] [Docker Registry 地址[:端口号]/] 仓库名[:标签] 具体选项可以从docker pull --help 命令查看，\nDocker镜像仓库地址：地址格式一般为 \u0026lt;域名/IP\u0026gt;[:端口号]。默认地址是 Docker Hub\n仓库名：仓库名是两段式，即\u0026lt;用户名\u0026gt;/\u0026lt;软件名\u0026gt;.对于Docker Hub，如果不给出用户名，默认为 library，也就是官方镜像\n$ docker pull ubuntu:18.04 上面命令没有给出Docker镜像仓库地址，默认从Docker Hub获取镜像。而镜像名称是ubuntun:18.04,因此会获取官方镜像 library/ubuntun仓库中标签为18.04的镜像\n2.运行 有了镜像后，我们就能够以这个镜像为基础启动并运行一个容器。以上面的ubuntu:18.04为例，如果我们打算启动ubuntu\u0026gt;\u0026gt;bash并且进行交互式操作的话，可以执行下面命令\n$ docker run -it --rm \\ ubuntu:18.04 \\ bash docker run就是运行容器的命令\n-it：是两个参数，一个是-i：交互式操作、一个是-t：终端。这里打算进入bash执行命令并查看返回结果， --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 --rm 可以避免浪费空间。 ubuntu:18.04：这是指用 ubuntu:18.04 镜像为基础来启动容器。 bash：放在镜像名后的是 命令，这里我们希望有个交互式 Shell，因此用的是 bash。 通过 exit 退出了这个容器。 列出镜像 使用docker image ls命令，可以列出已经下载下来的镜像\n$ docker image ls Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/ubuntu 18.04 e28a50f651f9 3 weeks ago 65.5 MB 列表包含了仓库名、标签、镜像ID、创建时间、所占用的空间\n镜像 ID 则是镜像的唯一标识，一个镜像可以对应多个 标签。\n1.镜像体积 docker image ls 列表中的镜像体积总和并非是所有镜像实际硬盘消耗。由于 Docker 镜像是多层存储结构，并且可以继承、复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 Docker 使用 Union FS，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多。\n可以使用 docker system df命令来查看镜像、容器、数据卷所占用的空间\n2.虚悬镜像 一个特殊的镜像，这个镜像既没有仓库名，也没有标签，均为 \u0026lt;none\u0026gt;。：\n\u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 00285df0df87 5 days ago 342 MB 这个镜像原本是有镜像名和标签的，原来为 mongo:3.2，随着官方镜像维护，发布了新版本后，重新 docker pull mongo:3.2 时，mongo:3.2 这个镜像名被转移到了新下载的镜像身上，而旧的镜像上的这个名称则被取消，从而成为了 \u0026lt;none\u0026gt;。除了 docker pull 可能导致这种情况，docker build 也同样可以导致这种现象。由于新旧镜像同名，旧镜像名称被取消，从而出现仓库名、标签均为 \u0026lt;none\u0026gt; 的镜像。这类无标签镜像也被称为 虚悬镜像(dangling image) ，可以用下面的命令专门显示这类镜像：\n$ docker image ls -f dangling=true REPOSITORY TAG IMAGE ID CREATED SIZE \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 00285df0df87 5 days ago 342 MB 一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。\n$ docker image prune 3.中间层镜像 为了加速镜像构建、重复利用资源，Docker会利用中间层镜像。使用在使用一段时间过后，可能会看到一些依赖的中间层镜像。默认的 docker image ls列表中只会显示顶层镜像，如果希望显示包括中间层镜像所在内的所有镜像的话，需要加-a参数\n$ docker image ls -a 4.列出部分镜像 不加任何参数的情况下，docker image ls 会列出所有顶层镜像，但是有时候我们只希望列出部分镜像。docker image ls 有好几个参数可以帮助做到这个事情。\n根据仓库名列出镜像\n$ docker image ls ubuntu REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 18.04 f753707788c5 4 weeks ago 127 MB ubuntu latest f753707788c5 4 weeks ago 127 MB 列出特定的某个镜像，也就是说指定仓库名和标签\n$ docker image ls ubuntu:18.04 REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 18.04 f753707788c5 4 weeks ago 127 MB 除此以外，docker image ls 还支持强大的过滤器参数 --filter，或者简写 -f。之前我们已经看到了使用过滤器来列出虚悬镜像的用法，它还有更多的用法。比如，我们希望看到在 mongo:3.2 之后建立的镜像，可以用下面的命令：\n$ docker image ls -f since=mongo:3.2 REPOSITORY TAG IMAGE ID CREATED SIZE redis latest 5f515359c7f8 5 days ago 183 MB nginx latest 05a60462f8ba 5 days ago 181 MB 想查看某个位置之前的镜像也可以，只需要把 since 换成 before 即可。\n此外，如果镜像构建时，定义了 LABEL，还可以通过 LABEL 来过滤。\n$ docker image ls -f label=com.example.version=0.1 ... 5.以特定格式显示 默认情况下，docker image ls 会输出一个完整的表格，但是我们并非所有时候都会需要这些内容。比如，刚才删除虚悬镜像的时候，我们需要利用 docker image ls 把所有的虚悬镜像的 ID 列出来，然后才可以交给 docker image rm 命令作为参数来删除指定的这些镜像，这个时候就用到了 -q 参数。\n","date":"2025-07-03","id":11,"permalink":"/notes/docker/docker/","summary":"Docker 一文","tags":"docker","title":"Docker(bate)"},{"content":" MySQL 内容代办事项\nSQL基础语法 MySQL架构 client层 server层 连接器 查询缓存 解析器 优化器 执行器 存储引擎 InnoDB 表空间 数据页结构 行记录存储 Buffer Pool MyISAM Memory MySQL索引 数据结构 B+树索引 Hash索引 Full-text 索引类型 聚簇索引 非聚簇索引 索引优化 主键索引 唯一索引 普通索引 联合索引 前缀索引 索引失效 索引覆盖 索引下推 索引选择 MySQL事务 ACID 原子性 隔离性 持久性 一致性 事务并发 脏读 不可重复读 幻读 事务隔离级别 读未提交 读已提交 可重复读 串行化 隔离级别实现 MVCC 锁 MySQL锁 全局锁 表级锁 表锁 MDL锁 意向锁 自增锁 局部锁 记录锁 间隙锁 临键锁 插入意见锁 死锁 MySQL日志 undo log redo log WAL 两阶段提交 bin log MySQL调优 explain 解释计划 查询性能调优 分页调优 连接池 MySQL高可用 读写分离 分库分表 分布式ID 分布式锁 数据迁移 分布式事务 高可用 分布式数据库 数据库设计 设计规范(三范式) 设计原则 反范式设计 ","date":"2025-07-02","id":12,"permalink":"/notes/database/mysql/todo/","summary":"MySQL 内容代办事项","tags":"MySQL","title":"MySQL - Todo"},{"content":" 一文详解 Redis 核心内容\n1. Redis 简介 1.1 Redis 安装 1.2 Redis 命令 1.3 Redis 高级 2. Redis 数据结构 2.1 Redis Object 2.2 Redis \u0026lt; String \u0026gt; 2.3 Redis \u0026lt; List \u0026gt; 2.4 Redis \u0026lt; Set \u0026gt; 2.5 Redis \u0026lt; Hash \u0026gt; 2.6 Redis \u0026lt; ZSet \u0026gt; 2.7 Redis \u0026lt; Stream \u0026gt; 2.8 Redis - Geospatial| Hyperloglog | Bitmap 3. Redis 架构设计 3.1 Redis 单线程 | 多线程 3.2 Redis 过期策略算法 4. Redis 持久化策略 4.1 Redis AOF 4.2 Redis RDB 4.3 Redis 混合持久化 5. Redis 缓存应用 5.1 Redis 缓存 5.2 Redis 缓存一致性 6. Redis 高可用 6.1 Redis 主从复制 6.2 Redis 哨兵 6.3 Redis 集群 Redis相关资料 redis/Docs 小林coding/redis redis中文网 菜鸟教程/redis zhihu/【超级详细】一文搞懂redis的所有知识点 ","date":"2025-06-10","id":13,"permalink":"/notes/database/redis/redis-core/","summary":"一文详解 Redis 核心内容","tags":"redis","title":"Redis-core"},{"content":" Hugo 是最受欢迎的开源静态网站生成器之一. 凭借其惊人的速度和灵活性，Hugo 让网站建设再次变得有趣.\n本篇内容将全部基于Hugo 官方文档以及个人的实际操作. 实际上Hugo 的官方文档非常加十分的详细，我也超级推荐直接阅读官方的文档. 但是官方文档的内容太多，这对新手来说第一时间很难上手，包括我，所以我想将整个文档进行阅读记录，以方便入门.\n于此同时，本站点也是使用了Hugo 进行搭建，所以进行这样一份学习记录也许有助于我改进网站.\nHugo Hugo是一个用Go语言编写的静态网站生成器，针对速度进行优化，并且灵活设计. 它凭借先进的模板系统和快速的资产管道，Hugo可以在几秒钟内渲染出完整的站点.\n其灵活的框架设计、多语言支持和强大的分类系统，Hugo很适合搭建文档站点，博客站点，以及各种静态网站.\n安装 - Installation 这里仅演示linux 下的安装过程\n在按照Hugo前，你需要先了解Hugo提供的三个版本：标准版、扩展版和扩展/部署版.\n标准版：仅包含核心功能，适合于开发者和普通用户使用. 扩展版：包含标准版所有功能，并且提供了许多扩展功能，如图片WebP格式处理, 使用嵌入的LibSass转化CSS, 使用Dart Sass转换器等. 扩展/部署版：包含扩展版所有功能，并且提供了许多部署功能，如Google Cloud Storage、AWS S3或者Azure存储容器等. Hugo官方推荐使用扩展版, 下面的安装过程也展示扩展版的安装.\n环境准备 使用Hugo时，Git, Go和Dart Sass是经常使用的. 其中Git是必须的，Go和Dart Sass如果不选择安装仅影响部分功能的使用.\n安装Git(必要) 安装Go(可选) 安装Dart Sass(可选) 直接下载 源码构建 仓库软件包 Configuration CLI ","date":"2025-04-08","id":14,"permalink":"/notes/hugo/hugo-docs/","summary":"Hugo 是最受欢迎的开源静态网站生成器之一. 凭借其惊人的速度和灵活性，Hugo 让网站建设再次变得有趣.","tags":"hugo","title":"Hugo Docs (Continuous Updates)"},{"content":" Bitcask是一个高性能的键值存储系统，设计之初的目的是提供高写入吞吐量和高效读取性能。采用了日志化结构哈希表(Log-Structured Hash Table)，核心是写前日志(WAL)、内存哈希表和定期merge.\n具体细节均在官方的设计论文 Bitcask Design Paper 中可以查看。\n官方论文中提到的bitcask应该具备的特性\nlow latency per item read or written high throughput, especially when writing an incoming stream of random items ability to handle datasets much larger than RAM w/o degradation crash friendliness, both in terms of fast recovery and not losing data ease of backup and restore a relatively simple, understandable (and thus supportable) code structure and data format predictable behavior under heavy access load or large volume a license that allowed for easy default use in Riak 目前已经有优秀的开源实现：\nprologic/bitcask ahmeducf/bitcask rosedblabs/mini-bitcask Bitcask 核心 目录结构(bitcask)：作为一个目录，存储数据文件，仅允许一个进程同时写入 数据文件(datafile)： activefile: 当前用于写入的文件，达到阈值（1GB）时关闭，变成仅读文件 readonlyfile: 仅读文件，存储旧数据条目 dataentry: 写入数据文件的数据条目 =\u0026gt; { crc | tstamp | key_sz | value_sz | key | value } 内存结构(keydir): 映射键值到数据文件中的位置，每次写入时原子更新 index: [key] -\u0026gt; { file_id | value_sz | value_pos | tstamp } 合并(merge): 定期或者主动触发合并操作，遍历不可变文件，创建新的包含最新现有键的数据文件，同时生成提示文件(hint) 提示文件(hint): 包含数据文件中值的位置和大小信息，即保存keydir，加速启动流程 恢复崩溃(recover): 由于数据文件的不可变且作为提交日志，恢复简单，扫描提示文件进行恢复 在bitcask论文中提到，允许处理比内存大得多的数据集，且不显著降低性能，是因为其采用了将键和元数据存储在内存中，而将数据值存储在磁盘上，通过键值分离的设计，使其能够处理比内存大得多的数据集。但是因此bitcask将所有的键存储在内存中，这可能在键数量极多的时候成为瓶颈。\n","date":"2025-04-05","id":15,"permalink":"/blogs/2025/0405.bitcask-core/","summary":"Bitcask是一个高性能的键值存储系统，设计之初的目的是提供高写入吞吐量和高效读取性能。采用了日志化结构哈希表(Log-Structured Hash Table)，核心是写前日志(WAL)、内存哈希表和定期merge.","tags":"bitcask","title":"Bitcask"},{"content":"关于本站 本站由 Hugo 搭建，使用math-queiroz/Rusty-Typewriter 扩展主题.\n计划在这里记录自己的学习笔记，包括但不限于 Golang, Java, Cangjie, cs_base, Git, Docker, Kubernetes, Message Queue, MySQL, Redis, HTML, CSS, JavaScript, TypeScript, Vue, React以及一些 开源项目 等\u0026hellip;\n文章结构 this is a example of directory structure content/ ├── archive │ ├── _index.md │ └── projects │ ├── bitcask.md │ └── _index.md ├── blogs │ ├── 2025-04-08.goose.md │ ├── bitcask │ │ ├── 2025-04-05.bitcask-core.md │ │ └── 2025-04-08.build-your-own-bitcask.md │ └── _index.md ├── hugoes │ ├── build-my-hugo-site.md │ └── hugo-docs.md ├── about.md └── search.md 在content/blogs 目录下，将以 date.title.md 的markdown文件列表. content/archive/**/* 将存在目录层级，以方便查找想要阅读的博客，\n计划未来支持 更加灵活的i18n支持 更多元的markdown支持 更加丰富的icon支持 更多丰富的组件支持 更详细的TODO列表 提供ctl+k打开搜索功能 修复不同语言下details_read_time错误 ==\u0026gt; 支持中文 修复归档文章的shortcode无法正常显示toc以及details_read_time 修复search搜索时查找错误 + 搜索失效问题 修复白天黑夜主题不点击主题按钮却发生主题切换的bug 添加归档详细页面 ","date":"2025-04-05","id":16,"permalink":"/about/","summary":"本站由 Hugo 搭建，使用math-queiroz/Rusty-Typewriter 扩展主题.","tags":"","title":"About"},{"content":"0. SQL语法 0.1 count主键和count非主键结果会不同吗？ 分析：count()函数是返回表中某个列的非NULL值数量\n主键不能存储NULL值，所以count(主键)返回的结果，可以表示数据库中所有行数据的量 非主键可以保存NULL值，所以count(非主键)返回表中非主键列的非NULL值的数量 回答： 主键不能存NULL值，所以count主键代表统计表中所有行数据的数量 而非主键可以存储NULL值，所以count非主键返回的是表中这个列的非NULL值的数量\n一、索引面试题 1.1 MySQL有哪些索引？ 分析：索引是由存储引擎来实现的，不同存储引擎支持的索引类型也是不同的，大多数存储引擎都是支持\nB+树索引，哈希索引，全文索引的区别：\nB+树索引：InnoDB引擎默认的索引，支持排序，分组，模糊查询等，并且性能稳定 哈希索引：多用于等值查询，时间复杂度为O(1)，效率非常高，但不支持排序，范围查询以及模糊查询 全文索引：一般用于查询文本中的关键字，而不是直接比较是否相等等，主要用来解决 WHERE name LIKE \u0026ldquo;%aaaa%\u0026rdquo; dev.mysql.com\n回答：我了解到Mysql支持B+树索引，哈希索引，全文索引这三种索引类型，比较常用的是B+树索引，因为它是InnoDB引擎默认使用的索引类型，支持排序，分组，范围查询，模糊查询等\n1.2 InnoDB引擎的索引数据结构是什么？ 回答：InnoDB引擎是采用B+树作为索引的数据结构\n1.3.0 mysql为什么使用B+树？ 分析：这里要回答对平衡树、红黑树，跳表，B树等的对比\n回答：\nB+树是多叉树，平衡二叉树、红黑树是二叉树，在同等数据量下，平衡二叉树、红黑树高度更高，磁盘IO次数更多，性能更差，而且它们会频繁执行在平衡过程，来保证树形结构平衡 和B+树相比，跳表在极端情况下会退化为链表，平衡性差，而数据库查询需要一个可预期的查询时间，并且跳表需要更多的内存 和B+树相比，B树的数据结构存储在全部节点，对范围查询不友好，非叶子节点存储了数据，导致内存中难以放下全部非叶子节点，如果内存放不下非叶子节点，那么意味着查询非叶子节点的时候都需要磁盘IO 学习：10｜数据库索引：为什么MySQL用B+树而不用B树？ | JUST DO IT\n1.3 为什么索引用B+树？而不用红黑树？ 分析：InnoDB引擎的数据是存储在磁盘上的，所以选择数据结构的第一优先级是考虑从磁盘查询数据的成本，如果树的高度越高，意味着磁盘I/O就越多，这样会影响查询性能\n对于N个叶子节点的B+树，其搜索复杂度为O(logdN) ，其中d表示节点允许的最大子节点个数为d\n在实际的应用中，即使数据达到了千万级别，B+树的高度依旧维持在34层，也就是说一次数据查询操作只需要做34次的磁盘I/O操作\n而红黑树本质上是二叉树，二叉树的每个父节点的儿子节点只能是2个，意味着其搜索复杂度为O(logN) ，这已经比B+Tree高出不少，因此二叉树搜索到目标数据所经历的磁盘I/O次数要更多\n回答：主要原因是随着数据量的增多，红黑树的树高会比B+树高 ，这样查询数据的时候会面临更多的磁盘I/O，查询性能没那么好。\n因为红黑树本质是二叉树，而b+树是多叉树，存储相同数量的数据量下，红黑树的树高会比B+树的树高，由于InnoDB引擎的数据都是存储在磁盘上的，如果树的高度过高，意味着磁盘I/O就越多，会影响到查询性能，所以InnoDB引擎的索引选择了B+树\n1.4 为什么索引用B+树？而不是B树？ 分析：考察对B+树 和 B 树的理解，可以从三个角度分析\n磁盘I/O角度\n范围查询角度\n增删改查角度\n回答：我觉得主要有三个原因：\nB+树的磁盘读写代价更低：B+树只有叶子节点存储索引和数据，非叶子节点只存放索引，而B树所有节点都会存放索引和数据，因此存储相同数据量的情况下，B+树可以比B树更矮胖，查询叶子节点的磁盘I/O次数也少\nB+树便于范围查询：MySQL经常需要使用范围查询，B+树所有叶子节点间都有链表进行连接，这种设计对范围查询查询非常有帮助，B树没有将所有叶子节点用链表串联起来的结构，只能用中序遍历来完成范围查询，这会比B+树范围查询涉及多个节点的磁盘I/O操作，一次范围查询的效率不如B+树\nB+树增删改查效率更加稳定：B+树有大量冗余节点，这些冗余数据可以让B+树在插入、删除的效率都更高，比如删除根节点的时候，不会像B树那样会发生复杂的树的变化。另外，B+树把所有指向数据的指针都放在叶子节点，因此查询、插入、删除数据都需要走到最后一层，这不同于B树可能在任意一层找到数据，所以B+树更为稳定\n1.5 为什么索引用B+树？而不用哈希表？ 分析：\n哈希表的数据是散列分布的，不具有序性，无法进行范围和排序\n哈希表存在哈希冲突，哈希冲突严重，也会降低查询效率\n回答：MySQL会有会多范围和排序的场景，虽然哈希表的搜索时间复杂度是O(1)，但是由于哈希表的数据都是通过哈希函数计算后散列分布的，所以哈希表索引不支持范围和排序操作，不支持联合索引最左匹配原则，如果重复键比较多，还容易操作哈希碰撞导致效率进一步降低。而B+树可以满足这些应用\n1.6聚簇索引和非聚簇索引有什么区别？ 分析：先说聚簇索引和非聚簇索引B+树叶子节点存放内容的区别，然后再引出回表查询和覆盖索引查询\n回答：聚簇索引和非聚簇索引（二级索引）的最主要区别是B+树叶子节点存放的内容：\n聚簇索引的B+树叶子节点存放的是主键值+完整的记录\n非聚簇索引的B+树叶子节点存放的是索引值+主键值\n如果查询条件用了二级索引，但查询的数据不是主键值，也不是二级索引值，这时在二级索引找到主键后，需要回表才能查找到数据，需要扫描两次B+树。如果查询的数据是主键值，因为在二级索引就能查询到，这时就会用到覆盖索引，不需要回表，只需要扫描一个B+树\n1.7insert操作对B+树结构的改变是怎么样的？ 分析：回答说出页分裂问题，以及指出主键id要是顺序递增，如果是随机值（比如uuid），就可能会频繁出现页分裂现象，会严重影响性能\n如果使用非自增主键，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有的数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂。页分裂还可能造成大量的内存碎片，导致所有结构不紧凑，从而影响查询效率\n回答：B+树的数据是有序的，所以：\n如果我们使用主键是顺序递增，那么每次插入新数据就会顺序插入到叶子节点最右边的节点里，如果页面满了，就会自动开辟一个新页面，将新数据插入到新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高\n如果使用主键不是顺序递增，由于每次插入主键的索引值都是随机的，因此每次插入新数据时，就可能插入到现有数据页中间的某个位置，这时候为了保证B+树的有序性，要移动其它数据来满足新数据的插入。如果该页面满了，就发生页分裂，这时候要从一个页面复制数据到另外一个页面，目的是保证后一个数据页的所有行主键值比前一个数据页中主键值大，页分裂可能会造成大量的内存碎片，导致所有结构不紧凑，从而影响查询效率\n所以，我们在设计主键的时候，最好采用自增的方式，或者顺序递增主键值\n1.8假如一张表有两千万的数据，B+树的高度是多少？怎么算？ 分析：假设\n非叶子节点内指向其它页的数量为x\n叶子节点内能容纳的数据行数为y\nB+树的层高为z\n表总数等于x的z-1次方与y的乘积：\n回答：具体看数据库表的字段多不多，以及字段类型，假设一行数据的大小是1kb，那么2000万的数据库，B+树的大概是三层高度\nMySQL 单表不要超过 2000W 行，靠谱吗？\n1.11 MySQL有哪些索引？ 分析：主键索引，唯一索引，普通索引，前缀索引，联合索引\n主键索引：主键索引是建立在主键字段上的索引，通常在创建表的时候一起创建，一张表最多只有一个主键，索引值不允许NULL值\n唯一索引：唯一索引建立在UNIQUE字段上的索引，一张表可以有多个唯一索引，索引列的值必须唯一，但是允许NULL值\n普通索引：普通索引是建立在普通字段上的索引，既不要求字段为主键，也不要求字段为UNIQUE\n前缀索引：前缀索引是指对字符串类型字段的前几个字符创建的索引，而不是在整个字段上建立的索引，前缀索引可以建在字段类型为char、varchar、binary、varbinary的列上，使用前缀索引的目的是为了减少索引占用的存储空间，提高查询效率\n联合索引：通过多个字段组合成一个索引，该索引被称为联合索引\n回答：MySQL有主键索引、唯一索引、前缀索引、普通索引和联合索引，InnoDB引擎要求每一张数据库表都必须有一个主键索引，比如表里的Id字段就是主键索引\n然后针对查询比较频繁的字段，我们可以对这个字段建立普通索引，如果是多个字段，可以考虑建立联合索引，利用索引覆盖的特性提高查询效率\n对长文本、字符串的字段，可以对这些字段的前缀部分建立索引，也就是建立前缀索引，可以减少索引的存储空间\n1.12 普通索引和唯一索引有什么区别？哪个更新性能更好？ 分析：从InnoDB的change buffer的角度分析\n回答：普通索引列的值是可以重复的，而唯一索引列的值是必须唯一的，当我们对唯一索引插入一条重复的值，会因为唯一性约束而报错\n我认为普通索引的更新性能更好，因为普通索引在更新的时候，如果更新的数据也不在内存的话，可以直接把更新操作缓存在change buffer中，更新就结束了；但是，唯一索引因为需要唯一性约束，如果更新的数据页不再内存的话，需要从磁盘读取对应的数据页到内存，判断有没有冲突，这涉及到磁盘随机IO的访问\n普通索引因为能使用change buffer特性，普通索引的更新比唯一索引，减少了随机磁盘访问，更新性能更好\n1.13 主键怎么设置？假如你不会设置会怎样？ 分析：在创建表的时候，指定某一列为主键（PRIMARY KEY）\nCREATE TABLE table_name ( ID INT PRIMARY KEY, ... ); InnoDB在创建聚簇索引的时候，会根据不同的场景选择不同的列作为索引：\n如果有主键，默认使用主键作为聚簇索引的索引值\n如果没有主键，就选择第一个唯一索引且不为NULL值的列作为聚簇索引的值\n在上面都没有的情况下，InnoDB将自动生成一个隐式自增id列作为聚簇索引的索引值\n回答：\n在创建表的时候，将id值设置为primary key，那么id列就是主键索引\n如果没有主键，那就选择第一个不包含NULL值的唯一索引作为聚簇索引的索引键，如果这个条件页没有，那么InnoDB将自动生成以一个隐式rowid列作为聚簇索引的索引键\n1.14 为什么要建立索引？ 分析：三个优点：\n索引大大减少了MySQL需要扫描的数据量\n索引可以帮助MySQL避免外部排序和使用临时表\n索引可以将随机I/O变成顺序I/O\n回答：如果没有建立索引，查询数据的话时间复杂度是O(n)，这样查询效率还是比较低，为了提高查询效率，可以建立索引\n建立索引后，数据都会按照顺序存储，这时候我们可以按照类似二分查找的方式快速查找数据，B+树索引是多叉树，搜索时间复杂度是O(logdN)，提高了查询效率，除此之外，可以避免外部排序和使用临时表问题，以及将随机I/O变成顺序I/O\n1.15 我们一般选择什么样的字段来建立索引？ 分析：\n适用索引的场景：\n字段具有唯一限制\n经常用于where 查询条件的字段，这样可以提高整个表的查询速度，有多条件还可以创建联合索引\n经常用于group by 和order by 的字段，这样查询的适合就不用再去做一次排序了，因为建立了索引之后在B+树中的记录都是排好序的\n不适合索引的场景：\nwhere 条件，group by 和order by 里用不到的字段，索引的价值是快速定位，起不到定位作用的字段通常不需要创建索引，因为索引是会占用空间\n字段中存在大量重复数据，不需要参加索引，例如性别字段\n经常更新的字段不用创建索引，比如不要对余额字段创建索引，因为要维护B+树的有序性，那么就需要频繁的重建索引，这个过程会影响数据库的性能\n回答：对于频繁用于where 查询条件的字段建立索引，可以提高整张表的查询速度，如果查询条件不是一个字段，可以考虑建立联合索引，还有对于经常用于排序、分组的字段建立索引\n对于一些区分度不高的字段，比如性别不建议建立索引，如果数据库中，数据的记录分布均匀，那么无论搜索的值是哪个都可能得到一半的数据，在这种情况下，MySQL的优化器发现某个值在表中出现的比例很高，它一般会忽略索引，进行全表扫描，这时候建立索引没有起到作用，而且所有还占用了存储空间\n1.16 索引越多越好吗？ 分析：考虑索引的缺点，索引的最大好处是提高查询效率，索引也有缺点\n空间代价：需要占用物理空间，数量越大，占用空间越大；\n时间代价：会降低表的增删改查的效率，每次删改索引，B+树为了维护索引有序性，都需要进行动态维护\n创建索引和维护索引要消耗时间，这种时间随着数据量的增大而增大\n回答：不是的。索引虽然能提高查询效率，但是多建立一个索引，就意味着新生成一个B+树索引，需要占据存储空间，特别是在表数据非常大的时候，索引占用的空间越大\n还有，索引越多数据库的写入性能会下降，因为每次对表进行增删改查的时候，都需要去维护各个B+树索引的有序性\n1.17 什么时候不用索引更好？ 回答：建立了索引，虽然可以提高查询效率，但是带来了两个代价，一个是空间代价，创建索引需要多构建一个b+树，会占用磁盘空间。第二个是时间代价，每次增删改查，都需要动态维护b+树，以满足b+树的有序性\n所以，在一张表经常增删改查的话，即读多写少的场景下，不建立索引会比较好，因为这时候维护的开销可能超过索引带来的性能提升\n还有一点，如果表中某个列的值高度重复，那么建立了索引页没有用，优化器会选择全表扫描，这样建立的索引会占用存储空间，也会影响增删改查的效率，选择不用索引更好\n1.18 索引怎么优化？ 回答：-\n对于只需要查询几个字段数据的sql来说，我们可以对这些字段建立联合索引，这样查询方式就变成了覆盖索引，避免回表，减少了大量的I/O操作\n主键值最好是递增的值，因为我们索引是按照顺序存储数据的，如果主键的值是随机的值，可能会引起页分裂的现象，页分裂会导致大量的内存碎片，这样索引结构不紧凑了，会影响查询效率\n避免出现索引失效的sql语句，比如不要对索引进行计算、函数、类型转发操作，联合索引要正确使用需要遵循最左匹配原则\n对于一些大字符串的索引，考虑用前缀索引只对索引列的前缀部分建立索引，节省索引的存储空间，提高查询效率\n1.19 建立了索引，查询的时候一定会用到索引吗？ 分析：两个方向：\n索引失效的场景\n优化器是基于成本考虑，即使查询条件用了索引，如果走索引的查询成本太高，也不会走索引\n回答：\n不是的\n了解到即使查询使用到了索引，也可能不走索引，比如\n当查询语句对索引字段进行左模糊匹配，表达式计算，函数，隐式类型转发操作，这时候查询语句就无法走索引了，查询方式变成了全表扫描，还有我们使用联合索引查询的时候，如果没有遵循最左匹配原则，也是会发生索引失效\n优化器是基于成本考虑来选择查询的方式，在使用二级索引进行查询的时候，优化器会计算回表的成本和全表扫描的成本，如果回表的代价太高，优化器会选择不走索引，而是走全表扫描\n11 索引出错:请理解 CBO 的工作原理\n1.20 定义了一个varchar类型的日期字段，并且有一个数据是'20230922\u0026rsquo;, 如果这个日期字段上有索引，那如果我查询的where条件是where time = 20230922不加单引号，还会命中索引吗？为什么？ 回答：不会命中索引，因为mysql在遇到字符串和数字时，会发生隐式类型转换，会将字符串转化为数字，这个转换的过程会涉及到函数，这个查询，日期字段是字符串，会发生隐式类型转换的时候，就会作用在日期这个索引字段上，对索引进行函数计算，发生索引失效\n1.21 MySQL最新版本解决了索引失效的哪些情况了吗？ 回答：mysql 8.0可以给字段增加函数索引，可以解决对索引使用函数的时候，索引失效的问题\n还有一个索引跳跃式扫描，即使没有遵循最左匹配原则，依然可以使用联合索引\n1.22 什么是最左匹配原则？ 回答：假设一个(a, b, c) 联合索引，它的存储顺序是先按照a排序，在a相同的情况下排b，在b相同的情况下排c，由于这个特性，在使用联合索引时，存在最左匹配原则，具体的规则是：\nmysql会从联合索引从最左的索引列开始匹配条件，然后依次从左到右顺序匹配，如果查询条件没有用到某个列，那么该列右边的索引列都无法使用走索引\n当查询条件中使用某个列，但是该列的值包含范围查询，范围查询的字段可以用到联合索引，但是范围查询字段后面的字段无法用到联合索引\nhttps://xiaolincoding.com/mysql/index/index_interview.html\n二、事务面试题 2.1 MySQL事务有什么特性？ 分析：考察事务的ACID特性\n原子性：一个事务中的所有操作，要么全部完成，要么全部不完成\n一致性：是指事务操作前后，数据满足完整性约束，数据库保持一致性状态\n隔离性：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以使得多个事务并发执行时不会相互干扰\n持久性：事务处理结束后，对数据的修改就是永久的，即便系统故障不会丢失\n回答：MySQL事务由ACID四大特性，分别是原子性、一致性、隔离性、持久性\n原子性就是事务中所有操作要么全部完成，要么全部不完成，不会结束在中间某个环节，原子性是由undo log日志保证的；\n一致性意思是事务执行前后，数据库的状态必须保持一致性，一致性是通过持久性+原子性+隔离性这三个共同保证；\n隔离性的意思是允许多个事务并发读写数据库，可以防止多个事务并发读写同一个数据库，导致数据不一致问题发生，隔离性是由mvcc和锁保证；\n持久性的意思是保证事务完成后对数据的修改是永久的，不会因为系统故障而丢失，持久性是由redo log日志来保证的；\n2.2 事务的隔离性如何保证？ 分析：先说由MVCC和锁实现，再说为什么用MVCC和锁能实现隔离性\n回答：事务的隔离性是由MVCC和锁来实现的\n可重复读隔离级别下的快照读（普通select），是通过MVCC来保证事务隔离性的，\n当前读（update、select \u0026hellip; for update）是通过行级锁来保证事务隔离性的\n2.3 事务的持久性如何保证？ 分析：先说由redo log实现的，再说为什么用redo log能实现持久性\n回答：事务的持久性是由redo log保证的，因为MySQL通过WAL（先写日志再写数据）机制，在修改数据时，会将本次对数据页的修改以redo log的形式记录下来，这时候更新操作就完成了，Buffer pool的脏页会通过后台线程刷盘，即使在脏页还没有刷盘的时候发生了数据库重启，由于修改操作记录到redo log，之前提交的记录都不会丢失，重启后在通过redo log，恢复脏页数据，从而保证事务的持久性\n2.4 事务的原子性如何保证？ 分析：先说由undo log实现的，再说为什么用undo log能实现原子性\n回答：事务的原子性是通过undo log实现的，在事务还没提交前，历史数据记录在undo log中，如果事务在执行过程中，出现了错误或者用户执行了ROLLBACK语句，MySQL可以利用undo log中的历史数据，将数据恢复到事务开始之前的状态，从而保证了事务的原子性\n2.5 MySQL事务和Redis事务有什么区别？ 分析：Redis事务没有保证原子性和持久性\n原子性：Redis事务没有回滚功能，没办法实现跟Mysql一样的原子性，如果在Redsi事务执行过程中，中间有命令出错，不会停止执行和回滚，这时候事务的执行会出现半成功的状态\n持久性：\nRDB模式：在一个事务执行后，而下一次的RDB快照还未执行前，如果发生了宕机，这种情况下，事务修改的数据也是不能保证持久的\nAOF模式：其三种选项no、everysec和always都会存在数据丢失，所以事务的持久性属性也还是得不到保证。\n回答：MySQL事务能够实现ACID四大特性，而Redis事务没保证原子性和持久性\nRedis事务没有回滚功能，没办法实现跟MySQL事务一样的原子性，就没办法保证事务执行期间，要不全部成功，要不全部成功，Redis事务执行过程中，如果中途有命令执行出错，不会停止和回滚，而是继续执行，那么就可能出现半成功的状态。\nRedis不管是AOF模式，还是RDB快照，都没办法保证数据不丢失，所以Redis事务不具有持久性\n2.6 MySQL事务隔离级别有哪些？分别解决哪些问题？ 分析：MySQL共有四个隔离级别如下：\n读未提交：指一个事务还没提交时，它做的变更就能被其他事务看到\n读提交：指一个事务提交之后，它做的变更才能被其他事务看到\n可重复读：指一个事务执行过程中看到的数据，一直跟着这个事务启动时看到的数据是一致的，MySQL InnoDB引擎的默认隔离级别\n串行化：会对记录加上锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行\n回答：MySQL默认隔离级别是可重复读。除此之外，MySQL的事务隔离级别有读未提交，读提交，可重复读，串行化\n事务并发问题存在脏读、不可重复读、幻读这三种，\n读未提交没有解决什么问题\n读提交解决了脏读，但是还存在不可重复读和幻读这两个问题\n可重复读解决了脏读，不可重复读的问题，不过对幻读问题上是很大程度避免了，没有完全避免\n串行化解决了脏读，不可重复读，幻读，但是事务的并发性是最差的\n2.7 脏读和幻读有什么区别？ 分析：考察脏读和幻读的基础\n回答：\n脏读是一个事务读取到了另一个未提交事务修改过的数据，如果另一个事务回滚了，刚才读到的数据就与数据库的数据不一致了\n幻读，是前后两次查询的结果集的数量是不同的，比如select执行了两次，但是第二次返回了第一次没有返回的行数据\n2.8 MySQL默认的隔离级别是什么？怎么实现？ 分析：考察可重复读的实现实现原理（MVCC和锁）\n回答：MySQL默认的隔离级别是可重复读\nselect查询是通过MVCC实现的，在MVCC实现中，每条记录都会保持多个版本，每个版本都有一个版本号，事务在读取数据时，会根据事务开始时的版本号来读取数据，从而保证了事务的隔离性。可重复读隔离级别是在开启事务后，执行一条select语句的时候，会生成一个Read View，后续事务查询数据的时候会复用Read View，所以保证了事务期间多次读取到的数据都是一致的。\n可重复读有间隙锁，针对当前读，会加间隙锁和记录锁，可以防止其他事务插入新记录，也可以防止删除和更新记录\n2.9 介绍以下MVCC？ 分析：从MVCC是什么？解决了什么问题？实现原理？三方面回答\n回答：MVCC是多版本并发控制，是通过记录历史版本数据，解决读写并发冲突问题，避免了读数据时加锁，提高了事务的并发性能\nMySQL将历史数据存储在undo log中，结构逻辑上类似链表，MySQL数据行上有两个隐藏列，一个是事务ID，另一个是指向undo log的指针\n事务开启后，执行第一个select语句的时候，会创建Read View，Read View记录了当前未提交的事务列表，通过与历史数据的事务ID比较，就可以根据可见性进行判断，判断这条记录是否可见，如果可见就直接将这个数据返回给客户端，如果不可见就继续往undo log版本链查找第一个可见的版本数据\n（扩展，讲可见性规则——\u0026gt; 2.10 MVCC如何判断行记录对某个事务是否可见？）\n2.10 MVCC如何判断行记录对某个事务是否可见？ 分析：Read View有四个重要的字段\nm_ids:指的是在创建Read View时，数据库中活跃事务的事务id列表，注意是一个列表\nmin_trx_id:指的是在创建Read View的时候，当前数据库中活跃的事务中id最小的事务\nmax_trx_id:知道是创建Read View时当前数据库中应该给下一个事务的id值，也就是当前全局事务中最大的事务id值+1；\ncreator_trx_id:指的是创建该Read View的事务的事务id\n记录中的两个隐藏列\ntrx_id:当前事务对记录进行改动后，会将该事务的id记录在trx_id\nroll_pointer:是一个指针，指向每一个旧版本记录（undo log）\n回答：每一条记录都有两个隐藏列，一个是事务id，另一个是指向历史数据undo log的指针，然后Read View有四个字段，分别是创建Read View的事务id，活跃事务id列表，活跃事务id列表中最小的id，下一个事务id\n当记录的事务id小于活跃事务id列表中最小的id，说明该记录是在创建Read View前提交好了，所以该记录是当前事务可见的\n当记录的事务id大于下一个事务的id，就说明该记录是在创建Read View后才生成的，所以该记录是当前事务是不可见的\n如果记录的事务id在最小的id和下一个事务id之间，这时候就需要判断记录的事务id是否在活跃的事务id列表中：\n如果该记录的事务id在活跃事务id列表中，说明该记录的事务还没提交，所以记录是不可见的\n不在活跃事务id列表中，说明该记录的事务已经提交，那么该记录是可见的\n2.11 读已提交和可重复读隔离级别实现 MVCC的区别？ 分析：生成readView的时机不同\n回答：读已提交和可重复读隔离级别都是由MVCC实现的，它们的区别在于创建Read View的时机不同\n读已提交隔离级别在事务开启后，每次执行select都会生成一个新的Read View，所以每次select都能看到其他事务最近提交的数据\n可重复读隔离级别在事务开启后，执行第一条select是生成一个Read View，然后整个事务期间都在复用这个Read View，所以一个事务执行过程中看到的数据，一直跟事务启动的时候看到的数据是一致的\n2.12 为什么互联网公司用读已提交隔离级别？ 分析：读已提交并发性能更高，因为读已提交没有间隙锁，只有记录锁，而可重复读是会有记录锁和间隙锁，所以读已提交隔离级别发生死锁的概率比较小\n回答：读已提交的并发性能更好，因为读已提交没有间隙锁，只有记录锁，发生死锁的概率比较低，然后互联网业务对于幻读和不可重复读的问题都能接受，所以为了降低死锁的概率，提高事务的并发性能，都会选择使用读已提交隔离级别\n2.13 可重复读级别是如何解决不可重复读的？ 分析：分两种查询来回答\n快照读，靠MVCC解决不可重复读\n当前读，靠行级锁中的记录锁解决不可重复读\n回答：MySQL提供了两种查询方式，一种是快照读，就是普通select语句，另外一种是当前读，比如 select for update语句。不同的查询方式，解决不可重复读问题方式不一样\n针对快照读：是通过MVCC实现的，在可重复读的隔离级别下，第一次select查询的时候，生成Read View，在第二次执行select的时候，会复用这个ReadView，这样前后两次查询的记录是一样的，不会读到其他事务更新的操作\n针对当前读：是靠行级锁中的记录锁来实现的，在可重复读隔离级别下，第一次select for update语句查询的时候，会对记录加next-key锁，这个锁包含记录锁，这个时候如果其他事务更新了加了锁的记录，都会被阻塞，这样就不会发生不可重复读\n2.14 可重复读隔离级别是如何解决幻读的？ 分析：分两种查询来回答\n快照读，靠MVCC解决幻读\n当前读，靠行级锁中的间隙锁解决幻读\n回答：MySQL提供了两种查询方式，一种是快照读，就是普通select语句，另外一种是当前读，比如select for update语句。不同的查询方式，解决不可重复读问题的方式不同\n快照读，是通过MVCC来解决的，在可重复读的隔离级别下，第一次执行select语句，会生成ReadView，在第二次执行select的时候，复用该ReadView，这样前后两次查询的结果集都是一样的，不会读到其他事务新插入的记录，这样就不会发生幻读\n当前读，看靠行级锁中的间隙锁来实现的，在可重复读隔离级别下，第一次select for update语句查询的时候，会对记录加next-key锁，这个锁包含间隙锁，这个时候如果其他事务往这个间隙插入新记录的化，都会被阻塞，这样就不会发生幻读\n2.15 可重复读隔离级别解决了什么问题？有没有完全解决幻读？ 分析；强调可重复读隔离级别是很大程度上解决了幻读，但是没有完全解决\n回答：可重复读解决了脏读，不可重复读问题，幻读在很大程度上避免了，但是并没有完全解决幻读，在一些特殊的场景，还是会发生幻读的问题。\n2.16 可重复读为什么不能完全避免幻读？什么情况下出现幻读？ 分析：可重复读隔离级别场景下：\n发生幻读的场景一：事务A中存在一条更新一条不存在的记录的语句，事务B在事务A更新之前insert这条不存在的记录并提交事务，事务A执行更新，能够正常执行且查询到这个记录\n发生幻读的场景二：范围查询\n回答：在可重复读隔离级别下，当先快照读后当前读的场景下可能会出现幻读的问题。\n比如这个场景，事务A通过快照读的方式查询了id=5的记录，此时数据库没有该记录，然后事务B向这张表新插入一条id=5的记录并提交了事务。接着，事务A对id=5的记录进行了更新操作，这个时刻，这条新纪录隐藏列中的事务id就变成了事务A的id，这时候事务A再使用select语句去查询这条记录时候就可以看到这条记录了，这里事务A前后两次查询的结果集合数不一样，于是就发生了幻读\n以上发生幻读的场景是可以避免的，就是尽量再开启事务后，马上执行select \u0026hellip; for update语句\n2.17 可重复读隔离级别，MVCC完全解决了不可重复读的问题吗？ 分析：不可重复读，代表前后两次查询的记录的值不一样\n比如表里面有 id = 1, value =1 的记录\n事务a, 先执行select，查询到 id =1的value = 1\n事务b，更新id =1的value =2 ，然后提交事务\n事务a，执行select for update，当前读，然后就读到 id= 1，value的记录，意味着发生了不可重复读\n回答：如果前后两次查询都是普通select，就不会产生不可重复读的问题。但是如果第一次查询是快照读，第二次查询是当前读，那么就可能会发生不可重复读\n三、锁面试题 3.1 细说一下MySQL数据库中锁的分类 分析：全局锁，表级锁，行级锁，强调InnoDB引擎实现了行级锁\n回答：根据颗粒度的不同，MySQL的锁可以分为全局锁，表级锁，行级锁。\n比较了解的是表级锁和行级锁，比如对一张表结构进行修改的时候，MySQL会对该表加一个元数据锁，元数据锁是属于表级锁\n行级锁目前只有InnoDB存储引擎实现了，MyISAM存储引擎是不支持行级锁，只有表锁。\nInnoDB存储引擎实现的是行级锁主要有记录锁，间隙锁，临键锁，插入意向锁这些\n3.2 在线上修改表结构，会发生什么？ 分析：表级锁\n回答：线上环境可能存在很多事务在读取这个表，如果这张表进行表结构的修改，会发生阻塞，原因是有事务对这张表进行读写操作，会发生元数据锁，而修改表结构的时候，会生成元数据写锁，这时候就发生了读写冲突，所以修改表结构的操作就会发生阻塞，并且后续事务的增删改查操作都会阻塞\n3.3 InnoDB存储引擎中的行级锁有哪些？ 分析：记录锁、间隙锁、临键锁、插入意向锁\n回答：InnoDB实现的行级锁有记录锁、间隙锁、临键锁和插入意向锁，在我们增删改或者锁定读语句的时候，都会对记录加行级锁\n3.4 一条Update语句没有带Where条件，加的是什么锁？ 分析：InnoDB加锁是索引加锁，可重复读级别下，加锁的基本单位是next-key锁，读已提交隔离级别下，加锁的级别单位是记录锁\n更新没有带where条件，会全表扫描，会对每天记录都加锁\n回答：\n可重复读的情况下：更新没有带where条件，会按照全表扫描，对每一条记录都加next-key锁，相当于锁住了全表\n读已提交隔离级别下，没有间隙锁，更新没有带where条件，是全表扫描，那么会对每条记录都加记录锁\n3.5 带了Where条件没有命中索引，加的是什么锁？ 同上\n3.6 两条更新语句更新同一条记录，加的是什么锁？ 分析：在可重复读的情况下，加锁的基本单位是next-key锁，但是在一些场景下会退化为记录锁或者间隙锁\n3.7 两条更新语句更新同一个跳记录的不同字段，加的是什么锁？ 分析：InnoDB加锁是加在行记录索引，不是针对更新字段的加锁\n3.8 MySQL怎么实现乐观锁？ 分析：可以基于版本号实现乐观锁，修改数据的时候带上版本号（时间戳）\nupdate student set name = \u0026#39;xiaochen\u0026#39;, version = 2 where id = 100 and version = 1 回答：可以在数据库表增加一个版本号字段，利用这个版本号字段实现乐观锁\n具体的实现，每次更新数据的时候，带上版本号，同时将版本+1，如果版本号和表记录中的版本号一致的话，就能更新成功，不相等就更新失败，然后重新获取该记录的最新版本号，然后再尝试更新数据\n3.9 了解过MySQL怎么排查死锁问题？ 回答：在并发事务中，当两个事务出现循环资源依赖，这两个事务都在等待别的事务释放资源，就会导致这两个事务进入无限等待的状态，这时就发生了死锁\n3.10 MySQL 怎么避免死锁？ 回答：\n在遇到线上死锁的问题，应该第一时间获取相关的死锁日志。使用show engine innodb status 命令获取死锁信息\n然后就分析死锁日志，\n通过阅读死锁日志，可以清楚知道两个事务形成了怎么样的循环等待，然后根据当前各个事务执行的sql分析出加锁类型以及顺序，逆向推断出如何形成循环等待\n解决死锁之路(终结篇) - 再见死锁 - aneasystone\u0026rsquo;s blog\nMySQL 5.7 排查死锁\nMySQL 死锁了，怎么办？\n四、日志面试题 4.1 redo log和 binlog 的区别和应用场景 分析：redo log和 binlog 有四个区别的地方\n适用的对象不同\nbinlog是MySQL的Server层实现的日志，所有的引擎都可以使用\nRedo log是InnoDB存储引擎实现的日志\n文件格式不同\nbinlog有3种格式类型，分别是STATEMENT、ROW、MIXED\nSTATEMENT：每一条修改过的SQL都会记录到binlog中， redo log是物理日志，记录的是在某个数据页做了什么修改，\n写入方式不同\nbinlog是追加写，写满一个文件，就创建一个文件继续写，不会覆盖以前的日志，保存的是全量的日志\nRedo log是循环写，日志空间大小是固定的，全部写满就从头开始，保存未被刷入磁盘的脏页日志\n用途不同：\nbinlog用于备份恢复，主从复制\nRedo log用于掉电等故障\n回答：\nredo log是InnoDB引擎实现的日志，属于物理日志，记录了InnoDB存储引擎对数据页所做的修改操作，主要用于崩溃恢复，例如某个事物提交了，脏页数据还可能没进行刷盘，如果mysql机器断电了，脏页的数据就丢失了，mysql重启后可以通过redo log将已提交事物的数据恢复过来\nbinlog是Server层实现的日志，保存了所有对数据库的增删改操作，binlog有三种日志格式，日志的内存可能是sql语句，数据本身或者两者混合，主要用于数据库的备份和归档，也用于主从复制\n4.2 redo log和binlog在恢复数据库有什么区别？ 分析：应用区别\n回答：\nbinlog是追加日志，写满一个日志，就创建一个文件继续写，不会覆盖以前的日志，保存了所有对数据库的更新操作，可以用来恢复某个时刻的数据或者全量恢复数据库数据\nRedo log是循环写，日志空间大小是固定的，全部写满就从头开始，保存的是InnoDB存储引擎对数据页所做的修改操作，用来恢复因中途MySQL断电丢失的脏页数据\n4.3 redo log是怎么实现持久化？ 回答：事务执行过程更新的数据，并不是在事务提交的时候，就把修改的数据刷入磁盘，而是修改buffer pool中数据页，并标记为脏页，然后在后台找到合适的时机进行刷盘\n如果事务提交了，脏页数据没有刷盘的时候，数据库发生宕机，就会导致事务修改的数据丢失\n所以MySQL引入了redo log，redo log保存的是物理日志，主要是记录InnoDB对某个数据页的修改操作，当事务提交的时候，redo log就先刷入磁盘，因为redo log保存了数据页的修改操作，即使脏数据没有刷盘时数据库发生了宕机，重启后MySQL通过重放redo log，就能恢复未刷盘的脏页，保证了数据的持久性\n4.4 redo log除了崩溃恢复还有什么其它作用 分析：\n回答：\n写redolog的方式是追加的形式，所以redolog写磁盘是一个顺序写的过程，而数据页是一个随机写的过程，顺序写的性能比随机写的性能高，事务在提交的时候，是先写日志在写数据的机制（WAL），相当于把mysql写入磁盘的操作从磁盘随机写变成了顺序写，所以redolog还可以提高MySQL写入磁盘的性能\n4.5 为什么需要两阶段提交 回答：\n两阶段提交是为了保证redolog和binlog逻辑一致，从而保证主从复制的时候不会出现数据不一致的问题。\n4.6 两阶段提交的过程？ 分析：\n回答：\n两个阶段，prepare阶段和commit阶段\nMySQL 日志：undo log、redo log、binlog 有什么用？\n六、存储引擎 6.1 执行一条查询sql的全过程 回答：Mysql执行一条查询语句的时候，会经过连接器、查询缓存、解析器、优化器、执行器和存储引擎这些模块\n首先，mysql的连接器会负责建立连接、校验用户权限，接收客户端的sql语句\n第二步，mysql会在查询缓存中查找数据，如果命中直接返回数据给客户端，否则就继续往下查询，不过这个查询缓存在mysql8.0版本就删除了，原因是只要对这张表进行了写操作，这张表的查询缓存就会失效，所以在实际场景中，查询缓存的命中率不高\n第三步、mysql的解析器会对sql语句进行词法分析和语法分析，构建语法树，方便后续模块读取表名，字段，语句类型\n第四步，mysql的优化器会基于查询成本的考虑，判断每个索引的执行成本，从中选择查询成本最小的执行计划\n第五步，mysql的执行器会根据执行计划来执行查询语句，从存储引擎读取记录，返回客户端\n6.2 mysql存储引擎有哪些？ 分析：mysql整体上分server层和存储引擎层\nServer层负责的部分是连接器、查询缓存、解析器、优化器、执行器\n存储引擎负责数据的读取和存储，存储引擎有InnoDB、MyISAM、Memory等\n回答：mysql常见的存储引擎有InnoDB、MyISAM、Memory\n比较熟悉的是InnoDB，它是MySQL默认的存储引擎，支持事物和行级锁，具有事物提交、回滚和崩溃恢复功能\nMyISAM不支持事物和行级锁，而且由于只支持表锁，锁的颗粒度大，更新性能比较差，比较适合读多写少的场景\nMemory了解较少，它是将数据存储在内存中，所以数据的读写比较快，但是数据不具备持久性，比较适用于临时存储数据的场景\n6.3 MyISAM和InnoDB存储引擎有什么区别 分析：从数据存储、B+树结构、锁粒度、事务四个角度分析\n数据存储：InnoDB引擎数据存储的方式采用的是索引组织表，在索引组织表中，数据即所有，索引即数据，因此表数据和索引数据都存储在同一个文件。MyISAM引擎数据存储的方式是采用堆表，在堆表的组织结构中，索引和数据是分开存储，因此表数据和索引数据分别放在两个不同的文件中存储\nB+树结构：InnoDB引擎B+树叶子节点存储索引+数据；MyISAM引擎B+树叶子节点存储索引+数据地址\n锁粒度：InnoDB引擎支持行级锁；而MyISAM不支持行级锁，仅支持表锁\n事务：InnoDB支持事务；MyISAM不支持事务\n回答：InnoDB引擎和MyISAM引擎在数据存储上不同，InnoDB引擎将表数据和索引数据存放在同一个文件，而MyISAM引擎将表数据和索引数据分开存储；所以，InnoDB引擎和B+树索引中的叶子节点存储的是索引和数据，MyISAM引擎B+树结构中的叶子节点存储的是索引和数据地。还有就是InnoDB引擎支持行级锁，MyISAM不支持行级锁，仅支持表锁\n6.4 用count(*)哪个存储引擎会更快？ 分析：InnoDB引擎执行count函数的时候，通过遍历的方式来统计记录个数，而MyISAM引擎执行count函数只需要O(1)复杂度，因为每张MyISAM的数据表都有一个meta信息存储了row_count值，由表级锁保证一致性，所以直接读取row_count的值就行\n而InnoDB存储引擎是支持事务的，同一时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB应该返回多少数据不确定，无法维护一个row_count变量\n回答：如果查询语句里面没有where查询条件，MyISAM引擎会比较快，因为MyISAM引擎的每张表会用一个变量存储表的总记录个数，执行count函数，直接返回这个变量就行。而InnoDB引擎执行count函数，需要遍历的方式来统计记录个数\n如果查询语句与where查询条件，MyISAM和InnoDB引擎执行count函数的时候，性能差不多\n6.5 NULL值是如何存储的？ 分析：MySQL 一行记录是怎么存储的？\n回答：MySQL行格式中会用NULL值列表来标记值为NULL的列，每个列对应一个二进制位，如果列的值为NULL，就会标记二进制位为1，否则为0，所以NULL值并不会存储在行格式中的真实数据部分\nNULL值表最少会占用1字节空间，当表中所有列都定义为NOT NULL，行格式中就不会有NULL值列表，这样可以省1字节空间\n6.6 char和varchar有什么区别？ 分析：MySQL中char与varchar的区别：存储机制、性能差异\n回答：\nChar 是固定长度的字符串类型，它在数据库中占用固定存储空间，无论实际存储的数据长度是多少，如果实际存储的字符串长度小于定义的长度，系统会自动使用空格填充。\nVarchar 是可变长度的字符串类型，实际存储时只占用实际字符串长度的空间，不会进行空格填充。\n设计数据库表的时候，大多数的时候是使用varchar可变长度的字符串类型，因为char会填充空格导致浪费存储空间，导致性能下降。因为char会多存储一些空格，意味着需要从磁盘多读写数据，会消耗更多的内存，还有查找数据时需要删除空格可能消耗一些cpu性能\n6.7 假如说一个字段是varchar(10)，但它其实只有6个字节，那么他在内存中的存储空间是多少？在文件中的存储空间是多少？ 分析：MySQL中varchar(10)和varchar(100)的优缺点 - 海布里Simple - 博客园\nvarchar是可变长字符串，保存到文件的时候，只会存储实际使用的字符串大小，但是内存会按照varchar最大值来固定分配大小\n回答：内存会占用10字节，文件存储空间会占用6字节，并且会额外使用1-2字节存储[可变长字符串长度]的空间\n6.8 如果硬件内存特别大，mysql缓存能否替代redis？ 分析：这里的mysql缓存代表用于缓存数据页的buffer pool，所以这个问题是，如果这个buffer pool无限大，能在内存装下所有数据，是否可以替代redis，从redis的优点跟mysql没有的点思考\n回答：不能替代\nmysql的所有模块，比如buffer pool、日志技术、事务并发模块都是面向磁盘页设计的，因此首要目标不是减少内存访问的代价，而是I/O访问的代价，所以内存访问代价并不是最优的选择，而redis是面向内存设计的数据库\nmysql在内存查询一个数据页的时候，都需要先查询页表，也就是走b+树的搜索过程，时间复杂度是O(logdN),而redis提供了多种的数据类型，比如Hash数据对象的时候，可以在O(1)时间复杂度查到数据\nmysql在更新数据的时候，mysql为了保证事务的隔离性，需要加锁，而redis更新操作都是不需要加锁的，还有Mysql为了保证事务的持久性，还需要刷盘redolog日志和binlog日志，Redis可以选择不持久化数据\n因此，即使buffer pool无限大，Mysql缓存的性能还是没有redis好\n既然有了innodb buffer pool为什么要有redis? - 知乎\nmysql基础篇\nhttps://mp.weixin.qq.com/s?__biz=Mzg5ODU2ODczMQ==\u0026mid=2247486128\u0026idx=1\u0026sn=afbd3ca37f4727db9d32460f98e73d5a\u0026chksm=c061cdc4f71644d228cad54d6a8395e27a46a9540f219a4f3776e5aa023608d131f9ec70b687#rd\nmysql原理篇\nhttps://mp.weixin.qq.com/s?__biz=Mzg5ODU2ODczMQ==\u0026mid=2247487888\u0026idx=1\u0026sn=973635eeaf7d1916e62bbceda72d27bd\u0026chksm=c061d6e4f7165ff271ee4bfc71de630c6538802b1d49c747bc0eb0be9264ae7619ed0648c47c#rd\nmysql性能篇\nhttps://mp.weixin.qq.com/s?__biz=Mzg5ODU2ODczMQ==\u0026mid=2247494731\u0026idx=1\u0026sn=e1bb64dd33c008cf46d5995044589228\u0026chksm=c0622b3ff715a229ef1831b468eb47b07aabd1edfb87637bf5c45408418dd37e3cf4ee9eca2e\u0026scene=178\u0026cur_album_id=2225658380164055048#rd\nMysql基础篇 1.Mysql是什么？ 回答：Mysql是一种传统的RDBM数据库，也就是关系型数据库，广泛应用于OLTP场景\n2.OLTP和OLAP的区别？ 回答：\nOLTP（联机事务处理）：是传统的关系型数据库的主要应用，用于基本的、日常的事务处理，例如银行的交易记录\nOLAP（联机分析处理）：是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供了直观易懂的查询结果。最常见的应用是复杂的动态报表系统\n总体来说：OLTP用于日常处理，OLAP用于数据分析\n3.数据库的三范式分别是什么？ 回答：\n第一范式：字段不可分。强调的是列的原子性，即数据库表的每一列都是不可分割的原子数据项\n第二范式：有主键，非主键字段依赖主键。\n第三范式：非主键字段不能互相依赖，任何非主键属性不依赖于其它非主属性\n4.DML是什么？ 回答：DML是数据操作语言，用于检索或者修改数据，平时最常使用的增删改查就是DML\n5.DDL是什么？ 回答：DDL是数据定义语言，用于操作数据结构，比如创建表，删除表，更改索引等都是DDL\n6.DCL是什么？ 回答：DCL是数据控制语言，用于定义数据库用户的权限，比如创建用户，授权用户，删除用户等都是DCL\n7.varchar与char的区别是什么？ 回答：char是一种固定长度的类型，varchar则是一种可变长度的类型。比如char(128)和varchar(128)，前者无论字符串长短，在磁盘上，都会占据固定的128字符大小，后者是可变长度，不过它的长度不超过128\n8.既然varchar是变长，那是不是设置varchar(1000)一定比varchar(100)好？ 回答：不是，虽然varchar是变长，在相同长度下，磁盘占用空间一样，将值设置更大，弹性空间也一样，但是这是由代价的，在内存加载的时候，每次都是按最大空间来分配的，显然，在排序场景，或者一些临时表聚合场景，更大的空间会产生明显的不利影响\n9.varchar是变长，char是定长，那能用varchar完全替代char么？ 回答：不能。varchar的优点是更灵活，但是char也有它的优势\n首先varchar会额外占用一个字节存储长度信息，而char则节省了一个字节；\n其次，char的存储空间是一次性分配的，存储是固定连续的，而varchar的存储的长度是可变的，当varchar更改前后的数据长度不一致时，就不可避免会出现碎片问题。针对此，需要进行碎片消除作业，也是额外的成本\n一般来说，长度固定的字段，还是用char比较合适，比如Hash，就很适合用char\n10.varchar(11)和int(11)中的11，有什么区别？ 回答：varchar中代表能存11个字符，int中只是代表显示长度，对大多数应用没有意义，只是规定一些工具用来显示字符的个数，比如int(1)和int(20)存储和计算其实是一样的\n11.delete和truncate的区别？ 回答：delete是删除行；truncate是整表删除。具体来说，有以下区别：\ntruncate之后，会释放空间；delete之后，不会释放空间，因为delete只是在行标记删除，后续可以复用；\ndelete因为是DML，会产生redo log；truncate是DDL则不会\ntruncate的效率更高\ntruncate之后，id从头开始；delete不会\n12.Mysql 有哪些存储引擎？ 回答：首先是InnoDB引擎，它提供了ACID事务的支持，并且还提供了行级锁和外键的约束。InnoDB的设计目的就是处理大数据容量的数据库系统\n还有MyIASM引擎，它是原本Mysql的默认引擎，不提供事务支持，也不支持行级锁和外键\n最后是一个MEMORY引擎，它的所有数据都是在内存中，数据的处理速度快，但是安全性不高，很少使用\n13.ACID是什么？ 回答：它是原子性、一致性、隔离性、持久性的缩写\n原子性：操作要么成功还是全失败\n一致性：数据执行前后保持一致\n隔离性：事务之间隔离，互不影响\n持久性：数据持久化，不会丢失\n14.主键和外键有什么区别？ 回答：主键是表中的一个或者多个字段，它的值用于唯一的标识表中的某一条记录\n外键是说某张表b的主键，在另外一张表a中被使用，那么a中该字段可以使用的范围取决于b。外键约束主要用来维护两张表之间数据的一致性\n15.那么一张表一定有主键吗？ 回答；是的，一定有。如果主动设置，则采用设置的，否则会自动生成一个默认的行\n16.你怎么查看有多少个SQL语句正在执行？ 回答：使用show processlist，它是显示用户正在运行的线程的命令。但是除非是root用户，或者进行了授权的用户，都只能看见自己正在运行的线程\n","date":"0001-01-01","id":17,"permalink":"/interviews/mysql/","summary":"分析：count()函数是返回表中某个列的非NULL值数量","tags":"","title":""}]